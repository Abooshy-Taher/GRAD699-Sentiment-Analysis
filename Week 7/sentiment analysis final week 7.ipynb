{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3B6Q_gdw3zQ"
      },
      "source": [
        "# Week 7 — Sentiment Analysis (Compute-Friendly, Colab-Ready)\n",
        "\n",
        "**Objective**: Predict sentiment (Negative / Neutral / Positive) from review text, and test the hypothesis that **time-of-day relates to negativity** and that **adding time-of-day features improves prediction** versus text-only.\n",
        "\n",
        "**Leakage Prevention**: All splitting is chronological and all transformations are fit **only on train**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYf9iHo7w3zU"
      },
      "outputs": [],
      "source": [
        "# === 1) Setup (installs, imports, seeds, folders) ===\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "# Disable W&B by default\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# === CONFIG / KNOBS ===\n",
        "STUDENT_SEED = 319302\n",
        "FAST_RUN = True\n",
        "SAMPLE_FRAC = 0.10           # if FAST_RUN True, sample chronologically from the beginning of train only\n",
        "MAX_ROWS = None              # optional hard cap\n",
        "MAX_SEQ_LEN_BERT = 256\n",
        "BERT_EPOCHS = 1\n",
        "BERT_BATCH = 16\n",
        "UNSLOTH_MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "UNSLOTH_MAX_SEQ_LEN = 256    # Reverted to 256 to match the model's internal reported max_sequence_length and fix the Dynamo ValueError\n",
        "UNSLOTH_TRAIN_MAX_ROWS = 20000\n",
        "UNSLOTH_VAL_MAX_ROWS = 2000\n",
        "UNSLOTH_TEST_EVAL_ROWS = 500\n",
        "UNSLOTH_MAX_STEPS = 1200\n",
        "UNSLOTH_PER_DEVICE_BATCH = 2\n",
        "UNSLOTH_GRAD_ACCUM = 8\n",
        "UNSLOTH_LR = 2e-4\n",
        "UNSLOTH_WARMUP_STEPS = 50\n",
        "UNSLOTH_LOGGING_STEPS = 25\n",
        "UNSLOTH_SAVE_STEPS = 200\n",
        "UNSLOTH_EVAL_STEPS = 200\n",
        "\n",
        "# Optional Drive output\n",
        "USE_DRIVE = False\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/GRAD699/Week7/\"\n",
        "\n",
        "# Install required packages (single clean cell)\n",
        "# Re-evaluate installation strategy for unsloth and bitsandbytes due to CUDA setup error.\n",
        "# The error \"Required library version not found: libbitsandbytes_cuda128.so\" indicates\n",
        "# that bitsandbytes is not correctly installed for the specific CUDA version in Colab.\n",
        "# A common fix is to uninstall existing and install unsloth with a direct CUDA spec.\n",
        "\n",
        "# First, uninstall potentially conflicting versions\n",
        "print(\"Uninstalling potentially conflicting packages...\")\n",
        "for pkg in [\"bitsandbytes\", \"trl\", \"xformers\", \"unsloth\"]:\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg])\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"{pkg} not found or already uninstalled.\")\n",
        "\n",
        "print(\"Installing unsloth, bitsandbytes, xformers, and trl with CUDA 12.1 compatibility...\")\n",
        "# Install unsloth from source with cu121 for CUDA 12.1, which is common on Colab A100 GPUs.\n",
        "# This ensures bitsandbytes and xformers are correctly compiled/linked.\n",
        "# We include trl directly here to ensure compatibility.\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-qqq\",\n",
        "                       \"unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git\",\n",
        "                       \"xformers\", \"trl\"])\n",
        "\n",
        "packages = [\n",
        "    \"pandas>=2.0.0\",\n",
        "    \"numpy>=1.24.0\",\n",
        "    \"matplotlib>=3.7.0\",\n",
        "    \"seaborn>=0.12.0\",\n",
        "    \"scikit-learn>=1.3.0\",\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"datasets>=2.18.0\",\n",
        "    \"evaluate>=0.4.1\",\n",
        "    \"accelerate>=0.20.0\",\n",
        "    # \"unsloth[colab-new]\", # Replaced by specific unsloth[cu121] installation\n",
        "    # \"trl<0.9.0\",          # Now part of the unsloth installation line\n",
        "    # \"bitsandbytes<0.43.0\", # Now handled by unsloth[cu121]\n",
        "]\n",
        "for pkg in packages:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "# Imports (after install)\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "# GPU check\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"⚠️  No GPU detected. Colab GPU is recommended.\")\n",
        "\n",
        "# Seeds for determinism\n",
        "random.seed(STUDENT_SEED)\n",
        "np.random.seed(STUDENT_SEED)\n",
        "torch.manual_seed(STUDENT_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(STUDENT_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Output folders\n",
        "OUTPUT_DIR = DRIVE_OUTPUT_DIR if USE_DRIVE else \"outputs\"\n",
        "FIGURES_DIR = os.path.join(OUTPUT_DIR, \"figures\")\n",
        "MODELS_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
        "TABLES_DIR = os.path.join(OUTPUT_DIR, \"tables\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(TABLES_DIR, exist_ok=True)\n",
        "\n",
        "print(\"✓ Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPJWt3Cyw3zW"
      },
      "source": [
        "### Optional: Mount Google Drive (disabled by default)\n",
        "If you want outputs saved to Google Drive, uncomment and run the cell below, then set `USE_DRIVE = True` in the setup cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n054KdnFw3zX"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGjyf-fdw3zX"
      },
      "source": [
        "## 2) Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn1Ktjnuw3zY"
      },
      "outputs": [],
      "source": [
        "def find_column(df, candidates):\n",
        "    for col in candidates:\n",
        "        if col in df.columns:\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "def load_amazon_data():\n",
        "    possible_paths = [\n",
        "        \"/content/Amazon_Data.csv\",\n",
        "        \"/content/drive/MyDrive/Amazon_Data.csv\",\n",
        "        \"Amazon_Data.csv\",\n",
        "    ]\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            df = pd.read_csv(path)\n",
        "            print(f\"✓ Loaded: {path}\")\n",
        "            return df\n",
        "    raise FileNotFoundError(\"Amazon_Data.csv not found in expected locations.\")\n",
        "\n",
        "df = load_amazon_data()\n",
        "\n",
        "# Detect and rename columns\n",
        "text_col = find_column(df, [\"text\", \"review\", \"reviewText\"])\n",
        "rating_col = find_column(df, [\"rating\", \"stars\", \"overall\"])\n",
        "time_col = find_column(df, [\"timestamp\", \"reviewTime\", \"time\"])\n",
        "\n",
        "if text_col is None or rating_col is None or time_col is None:\n",
        "    raise ValueError(\n",
        "        f\"Missing required columns. Found text={text_col}, rating={rating_col}, timestamp={time_col}\"\n",
        "    )\n",
        "\n",
        "df = df.rename(columns={text_col: \"text\", rating_col: \"rating\", time_col: \"timestamp\"})\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Head:\")\n",
        "print(df.head())\n",
        "print(\"Missingness summary:\")\n",
        "print(df[[\"text\", \"rating\", \"timestamp\"]].isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTlWuDdSw3zY"
      },
      "source": [
        "## 3) Clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6S5Utt3w3zY"
      },
      "outputs": [],
      "source": [
        "def parse_timestamp(series: pd.Series) -> pd.Series:\n",
        "    if pd.api.types.is_numeric_dtype(series):\n",
        "        max_val = series.max()\n",
        "        if max_val > 1e12:\n",
        "            return pd.to_datetime(series, errors=\"coerce\", unit=\"ms\")\n",
        "        if max_val > 1e9:\n",
        "            return pd.to_datetime(series, errors=\"coerce\", unit=\"s\")\n",
        "    return pd.to_datetime(series, errors=\"coerce\")\n",
        "\n",
        "df = df.dropna(subset=[\"text\", \"rating\", \"timestamp\"]).copy()\n",
        "df[\"timestamp\"] = parse_timestamp(df[\"timestamp\"])\n",
        "df = df.dropna(subset=[\"timestamp\"]).copy()\n",
        "\n",
        "df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
        "df = df[df[\"text\"].str.len() > 0].copy()\n",
        "\n",
        "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "# Optional hard cap (chronologically from start)\n",
        "if MAX_ROWS is not None:\n",
        "    df = df.head(MAX_ROWS).copy()\n",
        "\n",
        "# Helper columns\n",
        "df[\"review_len\"] = df[\"text\"].str.len()\n",
        "df[\"word_count\"] = df[\"text\"].str.split().str.len()\n",
        "\n",
        "print(\"Final shape:\", df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_KrADeKw3zZ"
      },
      "source": [
        "## 4) Create labels (3-class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaRvJ0WKw3zZ"
      },
      "outputs": [],
      "source": [
        "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "\n",
        "def rating_to_label(r):\n",
        "    if r <= 2:\n",
        "        return 0\n",
        "    if r == 3:\n",
        "        return 1\n",
        "    return 2\n",
        "\n",
        "df[\"label\"] = df[\"rating\"].apply(rating_to_label)\n",
        "df[\"label_name\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "print(df[\"label_name\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "casnR0eFw3za"
      },
      "source": [
        "## 5) Time-of-day features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h7GpJzuw3za"
      },
      "outputs": [],
      "source": [
        "df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "df[\"day_of_week\"] = df[\"timestamp\"].dt.dayofweek\n",
        "df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
        "\n",
        "# Daypart bins\n",
        "def daypart(hour):\n",
        "    if 0 <= hour <= 5:\n",
        "        return \"late_night\"\n",
        "    if 6 <= hour <= 11:\n",
        "        return \"morning\"\n",
        "    if 12 <= hour <= 16:\n",
        "        return \"afternoon\"\n",
        "    if 17 <= hour <= 21:\n",
        "        return \"evening\"\n",
        "    return \"night\"\n",
        "\n",
        "df[\"daypart\"] = df[\"hour\"].apply(daypart)\n",
        "df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
        "df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
        "\n",
        "print(\"✓ Time features created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nC3RFtbw3za"
      },
      "source": [
        "## 6) Chronological split (train/val/test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwbIdNojw3za"
      },
      "outputs": [],
      "source": [
        "def chronological_split(df, train_ratio=0.70, val_ratio=0.15):\n",
        "    n = len(df)\n",
        "    n_train = int(train_ratio * n)\n",
        "    n_val = int(val_ratio * n)\n",
        "    df_train = df.iloc[:n_train].copy()\n",
        "    df_val = df.iloc[n_train:n_train + n_val].copy()\n",
        "    df_test = df.iloc[n_train + n_val:].copy()\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "df_train, df_val, df_test = chronological_split(df)\n",
        "\n",
        "print(\"Train range:\", df_train[\"timestamp\"].min(), \"to\", df_train[\"timestamp\"].max())\n",
        "print(\"Val range:\", df_val[\"timestamp\"].min(), \"to\", df_val[\"timestamp\"].max())\n",
        "print(\"Test range:\", df_test[\"timestamp\"].min(), \"to\", df_test[\"timestamp\"].max())\n",
        "\n",
        "assert df_train[\"timestamp\"].max() <= df_val[\"timestamp\"].min(), \"Train/Val overlap\"\n",
        "assert df_val[\"timestamp\"].max() <= df_test[\"timestamp\"].min(), \"Val/Test overlap\"\n",
        "\n",
        "print(\"Sizes:\", len(df_train), len(df_val), len(df_test))\n",
        "print(\"Train label distribution:\")\n",
        "print(df_train[\"label_name\"].value_counts())\n",
        "print(\"Val label distribution:\")\n",
        "print(df_val[\"label_name\"].value_counts())\n",
        "print(\"Test label distribution:\")\n",
        "print(df_test[\"label_name\"].value_counts())\n",
        "\n",
        "# FAST_RUN: sample chronologically from the beginning of train only\n",
        "if FAST_RUN:\n",
        "    n_sample = max(1, int(len(df_train) * SAMPLE_FRAC))\n",
        "    df_train = df_train.head(n_sample).copy()\n",
        "    print(f\"FAST_RUN enabled: using first {n_sample} train rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8kA5tpqw3za"
      },
      "source": [
        "## 7) Visualizations (saved to outputs/figures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdi-1XHLw3zb"
      },
      "outputs": [],
      "source": [
        "# Plot A: count of reviews by hour\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.countplot(x=\"hour\", data=df_train, color=\"steelblue\")\n",
        "plt.title(\"Review Count by Hour (Train)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_a_count_by_hour.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot B: negativity rate by hour\n",
        "neg_by_hour = df_train.groupby(\"hour\")[\"label\"].apply(lambda x: (x == 0).mean()).reset_index()\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x=\"hour\", y=\"label\", data=neg_by_hour, color=\"salmon\")\n",
        "plt.title(\"Negativity Rate by Hour (Train)\")\n",
        "plt.ylabel(\"Negativity Rate\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_b_negativity_by_hour.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot C: negativity rate by daypart\n",
        "neg_by_daypart = df_train.groupby(\"daypart\")[\"label\"].apply(lambda x: (x == 0).mean()).reset_index()\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=\"daypart\", y=\"label\", data=neg_by_daypart, color=\"orange\")\n",
        "plt.title(\"Negativity Rate by Daypart (Train)\")\n",
        "plt.ylabel(\"Negativity Rate\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_c_negativity_by_daypart.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot D: heatmap of negativity rate (hour x day_of_week)\n",
        "heat = df_train.groupby([\"hour\", \"day_of_week\"])[\"label\"].apply(lambda x: (x == 0).mean()).reset_index()\n",
        "heat_pivot = heat.pivot(index=\"hour\", columns=\"day_of_week\", values=\"label\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(heat_pivot, cmap=\"Reds\", annot=False)\n",
        "plt.title(\"Negativity Rate Heatmap (Hour x Day of Week)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_d_heatmap_negativity.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot E: review length distribution by sentiment\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(x=\"label_name\", y=\"review_len\", data=df_train)\n",
        "plt.title(\"Review Length by Sentiment (Train)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_e_length_by_sentiment.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot F: review length distribution by daypart\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(x=\"daypart\", y=\"review_len\", data=df_train)\n",
        "plt.title(\"Review Length by Daypart (Train)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_f_length_by_daypart.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot G: rolling negativity rate over time\n",
        "daily = df_train.groupby(df_train[\"timestamp\"].dt.date)[\"label\"].apply(lambda x: (x == 0).mean()).reset_index()\n",
        "daily.columns = [\"date\", \"neg_rate\"]\n",
        "daily[\"rolling_neg\"] = daily[\"neg_rate\"].rolling(window=30, min_periods=5).mean()\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(daily[\"date\"], daily[\"rolling_neg\"], color=\"purple\")\n",
        "plt.title(\"Rolling Negativity Rate (30-day, Train)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_g_rolling_negativity.png\"), dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBmsW_dnw3zb"
      },
      "source": [
        "## 8) Baseline Model 1: TF-IDF (text-only) + Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa-7z4S4w3zb"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "metrics_rows = []\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=2)\n",
        "X_train = tfidf.fit_transform(df_train[\"text\"].values)\n",
        "X_val = tfidf.transform(df_val[\"text\"].values)\n",
        "X_test = tfidf.transform(df_test[\"text\"].values)\n",
        "\n",
        "y_train = df_train[\"label\"].values\n",
        "y_val = df_val[\"label\"].values\n",
        "y_test = df_test[\"label\"].values\n",
        "\n",
        "clf_text = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
        "clf_text.fit(X_train, y_train)\n",
        "\n",
        "val_pred = clf_text.predict(X_val)\n",
        "test_pred = clf_text.predict(X_test)\n",
        "\n",
        "val_acc = accuracy_score(y_val, val_pred)\n",
        "val_f1 = f1_score(y_val, val_pred, average=\"macro\")\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "test_f1 = f1_score(y_test, test_pred, average=\"macro\")\n",
        "\n",
        "print(\"Validation Report:\")\n",
        "print(classification_report(y_val, val_pred))\n",
        "print(\"Test Report:\")\n",
        "print(classification_report(y_test, test_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "            yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "plt.title(\"Confusion Matrix - TFIDF Text Only\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"cm_tfidf_text_only.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "metrics_rows.append({\n",
        "    \"model\": \"TFIDF_text_only\",\n",
        "    \"accuracy_test\": test_acc,\n",
        "    \"macro_f1_test\": test_f1,\n",
        "    \"notes\": \"Fast baseline, text only\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsUQGqqAw3zb"
      },
      "source": [
        "## 9) Baseline Model 2: TF-IDF (text) + Time Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x7HIw2Ew3zc"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Numeric time features\n",
        "num_features = [\"hour_sin\", \"hour_cos\", \"is_weekend\"]\n",
        "\n",
        "# Categorical time features\n",
        "cat_features = [\"day_of_week\", \"daypart\"]\n",
        "\n",
        "# Fix: Replace 'sparse=True' with 'sparse_output=True'\n",
        "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
        "enc.fit(df_train[cat_features])\n",
        "\n",
        "train_cat = enc.transform(df_train[cat_features])\n",
        "val_cat = enc.transform(df_val[cat_features])\n",
        "test_cat = enc.transform(df_test[cat_features])\n",
        "\n",
        "train_num = csr_matrix(df_train[num_features].values)\n",
        "val_num = csr_matrix(df_val[num_features].values)\n",
        "test_num = csr_matrix(df_test[num_features].values)\n",
        "\n",
        "X_train_time = hstack([train_num, train_cat])\n",
        "X_val_time = hstack([val_num, val_cat])\n",
        "X_test_time = hstack([test_num, test_cat])\n",
        "\n",
        "X_train_combined = hstack([X_train, X_train_time])\n",
        "X_val_combined = hstack([X_val, X_val_time])\n",
        "X_test_combined = hstack([X_test, X_test_time])\n",
        "\n",
        "clf_time = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
        "clf_time.fit(X_train_combined, y_train)\n",
        "\n",
        "val_pred2 = clf_time.predict(X_val_combined)\n",
        "test_pred2 = clf_time.predict(X_test_combined)\n",
        "\n",
        "val_acc2 = accuracy_score(y_val, val_pred2)\n",
        "val_f12 = f1_score(y_val, val_pred2, average=\"macro\")\n",
        "test_acc2 = accuracy_score(y_test, test_pred2)\n",
        "test_f12 = f1_score(y_test, test_pred2, average=\"macro\")\n",
        "\n",
        "print(\"Validation Report:\")\n",
        "print(classification_report(y_val, val_pred2))\n",
        "print(\"Test Report:\")\n",
        "print(classification_report(y_test, test_pred2))\n",
        "\n",
        "cm2 = confusion_matrix(y_test, test_pred2)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm2, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "            yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "plt.title(\"Confusion Matrix - TFIDF + Time Features\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"cm_tfidf_text_time.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "metrics_rows.append({\n",
        "    \"model\": \"TFIDF_text_time\",\n",
        "    \"accuracy_test\": test_acc2,\n",
        "    \"macro_f1_test\": test_f12,\n",
        "    \"notes\": \"Text + time features\"\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEiF5mf9w3zc"
      },
      "source": [
        "**Interpretation**: Compare TF-IDF text-only vs text+time. If the text+time model shows higher macro F1, time-of-day features add signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWiL6gl_w3zc"
      },
      "source": [
        "## 10) DistilBERT 3-class classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYRuosdRw3zc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers.training_args import IntervalStrategy # Import IntervalStrategy\n",
        "from datasets import Dataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LEN_BERT)\n",
        "\n",
        "train_ds = Dataset.from_pandas(df_train[[\"text\", \"label\"]])\n",
        "val_ds = Dataset.from_pandas(df_val[[\"text\", \"label\"]])\n",
        "test_ds = Dataset.from_pandas(df_test[[\"text\", \"label\"]])\n",
        "\n",
        "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
        "val_ds = val_ds.map(tokenize_fn, batched=True)\n",
        "test_ds = test_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "bert_args = TrainingArguments(\n",
        "    output_dir=os.path.join(MODELS_DIR, \"distilbert\"),\n",
        "    num_train_epochs=BERT_EPOCHS,\n",
        "    per_device_train_batch_size=BERT_BATCH,\n",
        "    per_device_eval_batch_size=BERT_BATCH,\n",
        "    # The 'evaluation_strategy' and 'save_strategy' parameters are causing a TypeError\n",
        "    # in the current environment/version combination. Removing them to proceed.\n",
        "    # evaluation_strategy=IntervalStrategy.EPOCH,\n",
        "    # save_strategy=IntervalStrategy.EPOCH,\n",
        "    logging_steps=50,\n",
        "    seed=STUDENT_SEED,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=bert_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "test_metrics = trainer.evaluate(test_ds)\n",
        "\n",
        "preds = trainer.predict(test_ds).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "\n",
        "cm_bert = confusion_matrix(y_test, pred_labels)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_bert, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "            yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "plt.title(\"Confusion Matrix - DistilBERT\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"cm_distilbert.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "metrics_rows.append({\n",
        "    \"model\": \"DistilBERT\",\n",
        "    \"accuracy_test\": test_metrics[\"eval_accuracy\"],\n",
        "    \"macro_f1_test\": test_metrics[\"eval_macro_f1\"],\n",
        "    \"notes\": \"Transformer classifier\"\n",
        "})\n",
        "\n",
        "model.save_pretrained(os.path.join(MODELS_DIR, \"distilbert\"))\n",
        "tokenizer.save_pretrained(os.path.join(MODELS_DIR, \"distilbert\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHwN0LSUw3zc"
      },
      "source": [
        "## 11) Unsloth SFT (compute-friendly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xogo6uSpw3zd"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer # Removed DataCollatorForCompletionOnlyLM for now, will let SFTTrainer pick default\n",
        "from transformers import TrainingArguments as SFTTrainingArguments\n",
        "from datasets import Dataset as HFDataset\n",
        "import os # Ensure os is imported for os.cpu_count()\n",
        "\n",
        "prompt_template = \"Classify sentiment as Negative, Neutral, or Positive.\\nReview: {review}\\nAnswer:\"\n",
        "\n",
        "def make_sft_text(text, label_id):\n",
        "    return f\"{prompt_template.format(review=text)}\\n{label_map[label_id]}\"\n",
        "\n",
        "# Limit text length before processing to ensure it fits within max_seq_len after tokenization\n",
        "# Estimate ~20 tokens for prompt/answer. Max review length should be around 230 tokens.\n",
        "# A rough character count, assuming ~4 chars per token.\n",
        "# Adjust MAX_CHARS_FOR_REVIEW to be more generous if needed, but the explicit tokenization below\n",
        "# will handle the hard truncation. This is just a pre-filter.\n",
        "MAX_CHARS_FOR_REVIEW = (UNSLOTH_MAX_SEQ_LEN - 30) * 3 # Rough estimate, just to prevent extremely long inputs\n",
        "\n",
        "train_sft = df_train.head(UNSLOTH_TRAIN_MAX_ROWS).copy().reset_index(drop=True)\n",
        "val_sft = df_val.head(UNSLOTH_VAL_MAX_ROWS).copy().reset_index(drop=True)\n",
        "test_sft = df_test.head(UNSLOTH_TEST_EVAL_ROWS).copy().reset_index(drop=True)\n",
        "\n",
        "# Truncate review text before forming SFT text to avoid excessively long strings\n",
        "train_sft[\"text\"] = train_sft[\"text\"].apply(lambda x: x[:MAX_CHARS_FOR_REVIEW])\n",
        "val_sft[\"text\"] = val_sft[\"text\"].apply(lambda x: x[:MAX_CHARS_FOR_REVIEW])\n",
        "test_sft[\"text\"] = test_sft[\"text\"].apply(lambda x: x[:MAX_CHARS_FOR_REVIEW])\n",
        "\n",
        "\n",
        "train_sft[\"formatted_text\"] = train_sft.apply(lambda r: make_sft_text(r[\"text\"], r[\"label\"]), axis=1)\n",
        "val_sft[\"formatted_text\"] = val_sft.apply(lambda r: make_sft_text(r[\"text\"], r[\"label\"]), axis=1)\n",
        "\n",
        "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "\n",
        "unsloth_model, unsloth_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=UNSLOTH_MODEL_NAME,\n",
        "    max_seq_length=UNSLOTH_MAX_SEQ_LEN,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Ensure pad_token and padding_side are set before tokenizing\n",
        "if unsloth_tokenizer.pad_token is None:\n",
        "    unsloth_tokenizer.pad_token = unsloth_tokenizer.eos_token\n",
        "unsloth_tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Tokenize datasets explicitly before passing to SFTTrainer\n",
        "def tokenize_function(examples):\n",
        "    # This will strictly truncate and pad to UNSLOTH_MAX_SEQ_LEN\n",
        "    tokenized_inputs = unsloth_tokenizer(\n",
        "        examples[\"formatted_text\"],\n",
        "        truncation=True,\n",
        "        max_length=UNSLOTH_MAX_SEQ_LEN,\n",
        "        padding=\"max_length\", # Crucial for Dynamo stability and consistent batch shapes\n",
        "    )\n",
        "    # For causal LM, labels are typically the input_ids shifted\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
        "    return tokenized_inputs\n",
        "\n",
        "train_dataset = HFDataset.from_pandas(train_sft[[\"formatted_text\"]], preserve_index=False)\n",
        "val_dataset = HFDataset.from_pandas(val_sft[[\"formatted_text\"]], preserve_index=False)\n",
        "\n",
        "# num_proc=os.cpu_count() can sometimes cause issues in Colab, revert to 1 if needed\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"formatted_text\"])\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"formatted_text\"])\n",
        "\n",
        "unsloth_model = FastLanguageModel.get_peft_model(\n",
        "    unsloth_model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=False,\n",
        "    random_state=STUDENT_SEED,\n",
        ")\n",
        "\n",
        "sft_args = SFTTrainingArguments(\n",
        "    output_dir=os.path.join(MODELS_DIR, \"unsloth\"),\n",
        "    per_device_train_batch_size=UNSLOTH_PER_DEVICE_BATCH,\n",
        "    per_device_eval_batch_size=UNSLOTH_PER_DEVICE_BATCH,\n",
        "    gradient_accumulation_steps=UNSLOTH_GRAD_ACCUM,\n",
        "    learning_rate=UNSLOTH_LR,\n",
        "    warmup_steps=UNSLOTH_WARMUP_STEPS,\n",
        "    max_steps=UNSLOTH_MAX_STEPS,\n",
        "    logging_steps=UNSLOTH_LOGGING_STEPS,\n",
        "    save_steps=UNSLOTH_SAVE_STEPS,\n",
        "    save_total_limit=2,\n",
        "    fp16=not use_bf16,\n",
        "    bf16=use_bf16,\n",
        "    optim=\"adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        "    seed=STUDENT_SEED,\n",
        ")\n",
        "\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=unsloth_model,\n",
        "    tokenizer=unsloth_tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    args=sft_args,\n",
        "    # dataset_text_field removed since we pre-tokenized\n",
        "    # max_seq_length also removed as it's applied during pre-tokenization and is less relevant here\n",
        "    packing=False,\n",
        "    # Data collator for causal language modeling, will handle label shifting and masking\n",
        "    # If not provided, SFTTrainer defaults to DataCollatorForLanguageModeling\n",
        "    # which is appropriate when dataset has input_ids and labels.\n",
        ")\n",
        "\n",
        "sft_trainer.train()\n",
        "FastLanguageModel.for_inference(unsloth_model)\n",
        "\n",
        "def parse_prediction(output_text: str):\n",
        "    try:\n",
        "        out = output_text.lower()\n",
        "        if \"negative\" in out:\n",
        "            return 0\n",
        "        if \"neutral\" in out:\n",
        "            return 1\n",
        "        if \"positive\" in out:\n",
        "            return 2\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "pred_rows = []\n",
        "correct = 0\n",
        "for _, row in test_sft.iterrows():\n",
        "    prompt = prompt_template.format(review=row[\"text\"])\n",
        "    inputs = unsloth_tokenizer([prompt], return_tensors=\"pt\", truncation=True, max_length=UNSLOTH_MAX_SEQ_LEN).to(unsloth_model.device)\n",
        "    outputs = unsloth_model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        max_new_tokens=3,\n",
        "        pad_token_id=unsloth_tokenizer.eos_token_id,\n",
        "    )\n",
        "    raw_output = unsloth_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    pred_id = parse_prediction(raw_output)\n",
        "    if pred_id is None:\n",
        "        pred_label = \"Unknown\"\n",
        "    else:\n",
        "        pred_label = label_map[pred_id]\n",
        "    if pred_id == row[\"label\"]:\n",
        "        correct += 1\n",
        "\n",
        "    pred_rows.append({\n",
        "        \"text\": row[\"text\"],\n",
        "        \"gold_label\": label_map[row[\"label\"]],\n",
        "        \"pred_label\": pred_label,\n",
        "        \"raw_output\": raw_output,\n",
        "    })\n",
        "\n",
        "unsloth_acc = correct / len(test_sft) if len(test_sft) > 0 else 0.0\n",
        "# Filter out unknown predictions for F1 score calculation\n",
        "filtered_pred_labels = [0 if r[\"pred_label\"] == \"Negative\" else 1 if r[\"pred_label\"] == \"Neutral\" else 2 if r[\"pred_label\"] == \"Positive\" else None for r in pred_rows]\n",
        "filtered_gold_labels = test_sft[\"label\"].values.tolist()\n",
        "\n",
        "# Remove None from both lists\n",
        "combined_filtered = [(g, p) for g, p in zip(filtered_gold_labels, filtered_pred_labels) if p is not None]\n",
        "if combined_filtered:\n",
        "    actual_gold_labels = [item[0] for item in combined_filtered]\n",
        "    actual_pred_labels = [item[1] for item in combined_filtered]\n",
        "    unsloth_f1 = f1_score(\n",
        "        actual_gold_labels,\n",
        "        actual_pred_labels,\n",
        "        labels=[0, 1, 2],\n",
        "        average=\"macro\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "else:\n",
        "    unsloth_f1 = 0.0\n",
        "\n",
        "\n",
        "metrics_rows.append({\n",
        "    \"model\": \"Unsloth_SFT\",\n",
        "    \"accuracy_test\": unsloth_acc,\n",
        "    \"macro_f1_test\": unsloth_f1,\n",
        "    \"notes\": \"SFT causal LM (compute-friendly)\"\n",
        "})\n",
        "\n",
        "unsloth_model.save_pretrained(os.path.join(MODELS_DIR, \"unsloth\"))\n",
        "unsloth_tokenizer.save_pretrained(os.path.join(MODELS_DIR, \"unsloth\"))\n",
        "\n",
        "pred_df = pd.DataFrame(pred_rows)\n",
        "pred_df.to_csv(os.path.join(TABLES_DIR, \"pred_examples.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7snFH0qDw3zd"
      },
      "source": [
        "## 12) Final comparison + report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbOB-53ow3zd"
      },
      "outputs": [],
      "source": [
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "print(metrics_df)\n",
        "\n",
        "# Save metrics\n",
        "metrics_df.to_csv(os.path.join(OUTPUT_DIR, \"metrics.csv\"), index=False)\n",
        "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics_rows, f, indent=2)\n",
        "\n",
        "# Report insights\n",
        "max_hour = int(neg_by_hour.loc[neg_by_hour[\"label\"].idxmax(), \"hour\"])\n",
        "max_daypart = neg_by_daypart.loc[neg_by_daypart[\"label\"].idxmax(), \"daypart\"]\n",
        "\n",
        "m1 = metrics_df[metrics_df[\"model\"] == \"TFIDF_text_only\"][\"macro_f1_test\"].values\n",
        "m2 = metrics_df[metrics_df[\"model\"] == \"TFIDF_text_time\"][\"macro_f1_test\"].values\n",
        "if len(m1) and len(m2):\n",
        "    diff = m2[0] - m1[0]\n",
        "else:\n",
        "    diff = 0.0\n",
        "\n",
        "final_report = f\"\"\"\n",
        "# Week 7 Final Report\n",
        "\n",
        "## RQ1: Does time-of-day relate to negativity?\n",
        "- Highest negativity by hour (train): hour={max_hour}\n",
        "- Highest negativity by daypart (train): {max_daypart}\n",
        "- Rolling negativity plot shows temporal drift patterns over time.\n",
        "\n",
        "## RQ2: Do time features improve prediction vs text-only?\n",
        "- TFIDF_text_only macro F1: {m1[0] if len(m1) else 'N/A'}\n",
        "- TFIDF_text_time macro F1: {m2[0] if len(m2) else 'N/A'}\n",
        "- Difference (time - text): {diff}\n",
        "\n",
        "## Compute tradeoffs\n",
        "- TF-IDF models are fast and CPU-friendly.\n",
        "- DistilBERT improves language understanding but is slower.\n",
        "- Unsloth SFT is compute-friendly with max_steps and small sequence length.\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"final_report.md\"), \"w\") as f:\n",
        "    f.write(final_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doOnnxiWw3zd"
      },
      "source": [
        "## 13) Zip outputs + download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DacWvah3w3ze"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "zip_path = shutil.make_archive(\"outputs_week7\", \"zip\", OUTPUT_DIR)\n",
        "print(\"Created:\", zip_path)\n",
        "print(\"Size (MB):\", os.path.getsize(zip_path) / (1024 * 1024))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwi5gMs3w3ze"
      },
      "outputs": [],
      "source": [
        "# Download in Colab\n",
        "from google.colab import files\n",
        "files.download(\"outputs_week7.zip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9aba05b"
      },
      "source": [
        "# Task\n",
        "Save the current notebook and download `outputs_week7.zip` and the notebook to upload to GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87aac7df"
      },
      "source": [
        "## Save Notebook\n",
        "\n",
        "### Subtask:\n",
        "Save the current state of the Google Colab notebook. This ensures all code changes are preserved before downloading.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b88766aa"
      },
      "source": [
        "#### Instructions\n",
        "1. Go to the 'File' menu in Google Colab.\n",
        "2. Select 'Save' or 'Save and pin revision' to ensure all your changes are preserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8322b770"
      },
      "source": [
        "## Confirm Output Archive\n",
        "\n",
        "### Subtask:\n",
        "Verify that the output archive ('outputs_week7.zip') has been created and is ready for download. This archive contains all figures, metrics, and model artifacts generated during the experiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19159dbf"
      },
      "source": [
        "## Download Files for GitHub\n",
        "\n",
        "### Subtask:\n",
        "Provide instructions for the user to download the saved notebook (`.ipynb`) and the `outputs_week7.zip` file, which can then be uploaded to GitHub.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3a7e402"
      },
      "source": [
        "### Subtask:\n",
        "Provide instructions for the user to download the saved notebook (`.ipynb`) and the `outputs_week7.zip` file, which can then be uploaded to GitHub.\n",
        "\n",
        "#### Instructions\n",
        "1. Download the notebook file by going to 'File' > 'Download' > 'Download .ipynb'.\n",
        "2. The `outputs_week7.zip` file should have already been downloaded to your local machine by the last executed cell, which contained the command `files.download(\"outputs_week7.zip\")`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2bc10ff"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize that the notebook and its generated outputs are prepared for posting to GitHub.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "091e0aca"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Instructions were provided for saving the current Google Colab notebook by selecting 'Save' or 'Save and pin revision' from the 'File' menu.\n",
        "*   Confirmation was made that the `outputs_week7.zip` archive was successfully created and ready for download.\n",
        "*   Instructions were given to the user for downloading the saved notebook by navigating to 'File' > 'Download' > 'Download .ipynb'.\n",
        "*   It was noted that the `outputs_week7.zip` file would have been automatically downloaded by a previous execution step via the `files.download(\"outputs_week7.zip\")` command.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The notebook and its generated outputs are now prepared for posting to GitHub.\n",
        "*   The process ensures the preservation and accessibility of both the analysis workflow and its resulting artifacts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2393145d"
      },
      "source": [
        "### Fix for 'state' key missing from 'metadata.widgets' error\n",
        "\n",
        "This code will read the current notebook, remove the problematic `metadata.widgets` entry, and provide a download link for the cleaned notebook. Run this cell, download the new `.ipynb` file, and try opening that version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d4c0bc9"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# NOTE: This cell assumes you have saved your notebook to Google Drive.\n",
        "# If you haven't, please go to File > Save a copy in Drive first.\n",
        "# And ensure your Google Drive is mounted if it's not already (using the 'Optional: Mount Google Drive' cell).\n",
        "\n",
        "# --- USER INPUT REQUIRED ---\n",
        "# Replace this with the actual, full path to this notebook file on your Google Drive.\n",
        "# You can find the path by right-clicking on your .ipynb file in the Colab file browser (left pane)\n",
        "# and selecting 'Copy path'.\n",
        "Week7 = \"\" # <<< REPLACE THIS\n",
        "\n",
        "if not notebook_path_on_drive:\n",
        "    print(\"ERROR: Please provide the 'notebook_path_on_drive' to proceed.\")\n",
        "else:\n",
        "    try:\n",
        "        # Read the original notebook file\n",
        "        with open(notebook_path_on_drive, 'r', encoding='utf-8') as f:\n",
        "            notebook_content = json.load(f)\n",
        "\n",
        "        # Remove the 'widgets' entry from the metadata if it exists\n",
        "        if 'metadata' in notebook_content and 'widgets' in notebook_content['metadata']:\n",
        "            del notebook_content['metadata']['widgets']\n",
        "            print(\"Removed 'metadata.widgets' entry.\")\n",
        "\n",
        "        # Save the modified content to a new file in the /content/ directory for download\n",
        "        cleaned_notebook_filename = \"cleaned_notebook.ipynb\"\n",
        "        with open(cleaned_notebook_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(notebook_content, f, indent=1)\n",
        "\n",
        "        print(f\"Cleaned notebook saved as '{cleaned_notebook_filename}'.\")\n",
        "        files.download(cleaned_notebook_filename)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Notebook file not found at {notebook_path_on_drive}. Please check the path.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error: Could not decode notebook as JSON. It might be corrupted or not a valid .ipynb file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}