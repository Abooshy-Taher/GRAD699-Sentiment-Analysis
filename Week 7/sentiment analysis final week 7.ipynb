{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 7 — Sentiment Analysis (Compute-Friendly, Colab-Ready)\n",
        "\n",
        "**Objective**: Predict sentiment (Negative / Neutral / Positive) from review text, and test the hypothesis that **time-of-day relates to negativity** and that **adding time-of-day features improves prediction** versus text-only.\n",
        "\n",
        "**Leakage Prevention**: All splitting is chronological and all transformations are fit **only on train**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1) Setup (installs, imports, seeds, folders) ===\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "# Disable W&B by default\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# === CONFIG / KNOBS ===\n",
        "STUDENT_SEED = 319302\n",
        "FAST_RUN = True\n",
        "SAMPLE_FRAC = 0.10           # if FAST_RUN True, sample chronologically from the beginning of train only\n",
        "MAX_ROWS = None              # optional hard cap\n",
        "MAX_SEQ_LEN_BERT = 256\n",
        "BERT_EPOCHS = 1\n",
        "BERT_BATCH = 16\n",
        "\n",
        "# Optional Drive output\n",
        "USE_DRIVE = False\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/GRAD699/Week7/\"\n",
        "\n",
        "# Install required packages (single clean cell)\n",
        "packages = [\n",
        "    \"pandas>=2.0.0\",\n",
        "    \"numpy>=1.24.0\",\n",
        "    \"matplotlib>=3.7.0\",\n",
        "    \"seaborn>=0.12.0\",\n",
        "    \"scikit-learn>=1.3.0\",\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"datasets>=2.18.0\",\n",
        "    \"evaluate>=0.4.1\",\n",
        "    \"accelerate>=0.20.0\",\n",
        "]\n",
        "for pkg in packages:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "# Imports (after install)\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "# GPU check\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"⚠️  No GPU detected. Colab GPU is recommended.\")\n",
        "\n",
        "# Seeds for determinism\n",
        "random.seed(STUDENT_SEED)\n",
        "np.random.seed(STUDENT_SEED)\n",
        "torch.manual_seed(STUDENT_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(STUDENT_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Output folders\n",
        "OUTPUT_DIR = DRIVE_OUTPUT_DIR if USE_DRIVE else \"outputs\"\n",
        "FIGURES_DIR = os.path.join(OUTPUT_DIR, \"figures\")\n",
        "MODELS_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
        "TABLES_DIR = os.path.join(OUTPUT_DIR, \"tables\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(TABLES_DIR, exist_ok=True)\n",
        "\n",
        "print(\"✓ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Mount Google Drive (disabled by default)\n",
        "If you want outputs saved to Google Drive, uncomment and run the cell below, then set `USE_DRIVE = True` in the setup cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_column(df, candidates):\n",
        "    for col in candidates:\n",
        "        if col in df.columns:\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "def load_amazon_data():\n",
        "    possible_paths = [\n",
        "        \"/content/Amazon_Data.csv\",\n",
        "        \"/content/drive/MyDrive/Amazon_Data.csv\",\n",
        "        \"Amazon_Data.csv\",\n",
        "    ]\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            df = pd.read_csv(path)\n",
        "            print(f\"✓ Loaded: {path}\")\n",
        "            return df\n",
        "    raise FileNotFoundError(\"Amazon_Data.csv not found in expected locations.\")\n",
        "\n",
        "df = load_amazon_data()\n",
        "\n",
        "# Detect and rename columns\n",
        "text_col = find_column(df, [\"text\", \"review\", \"reviewText\"])\n",
        "rating_col = find_column(df, [\"rating\", \"stars\", \"overall\"])\n",
        "time_col = find_column(df, [\"timestamp\", \"reviewTime\", \"time\"])\n",
        "\n",
        "if text_col is None or rating_col is None or time_col is None:\n",
        "    raise ValueError(\n",
        "        f\"Missing required columns. Found text={text_col}, rating={rating_col}, timestamp={time_col}\"\n",
        "    )\n",
        "\n",
        "df = df.rename(columns={text_col: \"text\", rating_col: \"rating\", time_col: \"timestamp\"})\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Head:\")\n",
        "print(df.head())\n",
        "print(\"Missingness summary:\")\n",
        "print(df[[\"text\", \"rating\", \"timestamp\"]].isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_timestamp(series: pd.Series) -> pd.Series:\n",
        "    if pd.api.types.is_numeric_dtype(series):\n",
        "        max_val = series.max()\n",
        "        if max_val > 1e12:\n",
        "            return pd.to_datetime(series, errors=\"coerce\", unit=\"ms\")\n",
        "        if max_val > 1e9:\n",
        "            return pd.to_datetime(series, errors=\"coerce\", unit=\"s\")\n",
        "    return pd.to_datetime(series, errors=\"coerce\")\n",
        "\n",
        "df = df.dropna(subset=[\"text\", \"rating\", \"timestamp\"]).copy()\n",
        "df[\"timestamp\"] = parse_timestamp(df[\"timestamp\"])\n",
        "df = df.dropna(subset=[\"timestamp\"]).copy()\n",
        "\n",
        "df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
        "df = df[df[\"text\"].str.len() > 0].copy()\n",
        "\n",
        "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "# Optional hard cap (chronologically from start)\n",
        "if MAX_ROWS is not None:\n",
        "    df = df.head(MAX_ROWS).copy()\n",
        "\n",
        "# Helper columns\n",
        "df[\"review_len\"] = df[\"text\"].str.len()\n",
        "df[\"word_count\"] = df[\"text\"].str.split().str.len()\n",
        "\n",
        "print(\"Final shape:\", df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Create labels (3-class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "\n",
        "def rating_to_label(r):\n",
        "    if r <= 2:\n",
        "        return 0\n",
        "    if r == 3:\n",
        "        return 1\n",
        "    return 2\n",
        "\n",
        "df[\"label\"] = df[\"rating\"].apply(rating_to_label)\n",
        "df[\"label_name\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "print(df[\"label_name\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Time-of-day features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "df[\"day_of_week\"] = df[\"timestamp\"].dt.dayofweek\n",
        "df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
        "\n",
        "# Daypart bins\n",
        "def daypart(hour):\n",
        "    if 0 <= hour <= 5:\n",
        "        return \"late_night\"\n",
        "    if 6 <= hour <= 11:\n",
        "        return \"morning\"\n",
        "    if 12 <= hour <= 16:\n",
        "        return \"afternoon\"\n",
        "    if 17 <= hour <= 21:\n",
        "        return \"evening\"\n",
        "    return \"night\"\n",
        "\n",
        "df[\"daypart\"] = df[\"hour\"].apply(daypart)\n",
        "df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
        "df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
        "\n",
        "print(\"✓ Time features created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Chronological split (train/val/test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chronological_split(df, train_ratio=0.70, val_ratio=0.15):\n",
        "    n = len(df)\n",
        "    n_train = int(train_ratio * n)\n",
        "    n_val = int(val_ratio * n)\n",
        "    df_train = df.iloc[:n_train].copy()\n",
        "    df_val = df.iloc[n_train:n_train + n_val].copy()\n",
        "    df_test = df.iloc[n_train + n_val:].copy()\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "df_train, df_val, df_test = chronological_split(df)\n",
        "\n",
        "print(\"Train range:\", df_train[\"timestamp\"].min(), \"to\", df_train[\"timestamp\"].max())\n",
        "print(\"Val range:\", df_val[\"timestamp\"].min(), \"to\", df_val[\"timestamp\"].max())\n",
        "print(\"Test range:\", df_test[\"timestamp\"].min(), \"to\", df_test[\"timestamp\"].max())\n",
        "\n",
        "assert df_train[\"timestamp\"].max() <= df_val[\"timestamp\"].min(), \"Train/Val overlap\"\n",
        "assert df_val[\"timestamp\"].max() <= df_test[\"timestamp\"].min(), \"Val/Test overlap\"\n",
        "\n",
        "print(\"Sizes:\", len(df_train), len(df_val), len(df_test))\n",
        "print(\"Train label distribution:\")\n",
        "print(df_train[\"label_name\"].value_counts())\n",
        "print(\"Val label distribution:\")\n",
        "print(df_val[\"label_name\"].value_counts())\n",
        "print(\"Test label distribution:\")\n",
        "print(df_test[\"label_name\"].value_counts())\n",
        "\n",
        "# FAST_RUN: sample chronologically from the beginning of train only\n",
        "if FAST_RUN:\n",
        "    n_sample = max(1, int(len(df_train) * SAMPLE_FRAC))\n",
        "    df_train = df_train.head(n_sample).copy()\n",
        "    print(f\"FAST_RUN enabled: using first {n_sample} train rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Visualizations (saved to outputs/figures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot A: count of reviews by hour\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.countplot(x=\"hour\", data=df_train, color=\"steelblue\")\n",
        "plt.title(\"Review Count by Hour (Train)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_a_count_by_hour.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot B: negativity rate by hour\n",
        "neg_by_hour = df_train.groupby(\"hour\")[\"label\"].apply(lambda x: (x == 0).mean()).reset_index()\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x=\"hour\", y=\"label\", data=neg_by_hour, color=\"salmon\")\n",
        "plt.title(\"Negativity Rate by Hour (Train)\")\n",
        "plt.ylabel(\"Negativity Rate\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_b_negativity_by_hour.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot C: negativity rate by daypart\n",
        "neg_by_daypart = df_train.groupby(\"daypart\")[\"label\"].apply(lambda x: (x == 0).mean()).reset_index()\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=\"daypart\", y=\"label\", data=neg_by_daypart, color=\"orange\")\n",
        "plt.title(\"Negativity Rate by Daypart (Train)\")\n",
        "plt.ylabel(\"Negativity Rate\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_c_negativity_by_daypart.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot D: heatmap of negativity rate (hour x day_of_week)\n",
        "heat = df_train.groupby([\"hour\", \"day_of_week\"])[\"label\"].apply(lambda x: (x == 0).mean()).reset_index()\n",
        "heat_pivot = heat.pivot(index=\"hour\", columns=\"day_of_week\", values=\"label\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(heat_pivot, cmap=\"Reds\", annot=False)\n",
        "plt.title(\"Negativity Rate Heatmap (Hour x Day of Week)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_d_heatmap_negativity.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot E: review length distribution by sentiment\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(x=\"label_name\", y=\"review_len\", data=df_train)\n",
        "plt.title(\"Review Length by Sentiment (Train)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_e_length_by_sentiment.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot F: review length distribution by daypart\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(x=\"daypart\", y=\"review_len\", data=df_train)\n",
        "plt.title(\"Review Length by Daypart (Train)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_f_length_by_daypart.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot G: rolling negativity rate over time\n",
        "daily = df_train.groupby(df_train[\"timestamp\"].dt.date)[\"label\"].apply(lambda x: (x == 0).mean()).reset_index()\n",
        "daily.columns = [\"date\", \"neg_rate\"]\n",
        "daily[\"rolling_neg\"] = daily[\"neg_rate\"].rolling(window=30, min_periods=5).mean()\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(daily[\"date\"], daily[\"rolling_neg\"], color=\"purple\")\n",
        "plt.title(\"Rolling Negativity Rate (30-day, Train)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"plot_g_rolling_negativity.png\"), dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Baseline Model 1: TF-IDF (text-only) + Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "metrics_rows = []\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=2)\n",
        "X_train = tfidf.fit_transform(df_train[\"text\"].values)\n",
        "X_val = tfidf.transform(df_val[\"text\"].values)\n",
        "X_test = tfidf.transform(df_test[\"text\"].values)\n",
        "\n",
        "y_train = df_train[\"label\"].values\n",
        "y_val = df_val[\"label\"].values\n",
        "y_test = df_test[\"label\"].values\n",
        "\n",
        "clf_text = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
        "clf_text.fit(X_train, y_train)\n",
        "\n",
        "val_pred = clf_text.predict(X_val)\n",
        "test_pred = clf_text.predict(X_test)\n",
        "\n",
        "val_acc = accuracy_score(y_val, val_pred)\n",
        "val_f1 = f1_score(y_val, val_pred, average=\"macro\")\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "test_f1 = f1_score(y_test, test_pred, average=\"macro\")\n",
        "\n",
        "print(\"Validation Report:\")\n",
        "print(classification_report(y_val, val_pred))\n",
        "print(\"Test Report:\")\n",
        "print(classification_report(y_test, test_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "            yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "plt.title(\"Confusion Matrix - TFIDF Text Only\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"cm_tfidf_text_only.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "metrics_rows.append({\n",
        "    \"model\": \"TFIDF_text_only\",\n",
        "    \"accuracy_test\": test_acc,\n",
        "    \"macro_f1_test\": test_f1,\n",
        "    \"notes\": \"Fast baseline, text only\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Baseline Model 2: TF-IDF (text) + Time Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Numeric time features\n",
        "num_features = [\"hour_sin\", \"hour_cos\", \"is_weekend\"]\n",
        "\n",
        "# Categorical time features\n",
        "cat_features = [\"day_of_week\", \"daypart\"]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
        "enc.fit(df_train[cat_features])\n",
        "\n",
        "train_cat = enc.transform(df_train[cat_features])\n",
        "val_cat = enc.transform(df_val[cat_features])\n",
        "test_cat = enc.transform(df_test[cat_features])\n",
        "\n",
        "train_num = csr_matrix(df_train[num_features].values)\n",
        "val_num = csr_matrix(df_val[num_features].values)\n",
        "test_num = csr_matrix(df_test[num_features].values)\n",
        "\n",
        "X_train_time = hstack([train_num, train_cat])\n",
        "X_val_time = hstack([val_num, val_cat])\n",
        "X_test_time = hstack([test_num, test_cat])\n",
        "\n",
        "X_train_combined = hstack([X_train, X_train_time])\n",
        "X_val_combined = hstack([X_val, X_val_time])\n",
        "X_test_combined = hstack([X_test, X_test_time])\n",
        "\n",
        "clf_time = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
        "clf_time.fit(X_train_combined, y_train)\n",
        "\n",
        "val_pred2 = clf_time.predict(X_val_combined)\n",
        "test_pred2 = clf_time.predict(X_test_combined)\n",
        "\n",
        "val_acc2 = accuracy_score(y_val, val_pred2)\n",
        "val_f12 = f1_score(y_val, val_pred2, average=\"macro\")\n",
        "test_acc2 = accuracy_score(y_test, test_pred2)\n",
        "test_f12 = f1_score(y_test, test_pred2, average=\"macro\")\n",
        "\n",
        "print(\"Validation Report:\")\n",
        "print(classification_report(y_val, val_pred2))\n",
        "print(\"Test Report:\")\n",
        "print(classification_report(y_test, test_pred2))\n",
        "\n",
        "cm2 = confusion_matrix(y_test, test_pred2)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm2, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "            yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "plt.title(\"Confusion Matrix - TFIDF + Time Features\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"cm_tfidf_text_time.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "metrics_rows.append({\n",
        "    \"model\": \"TFIDF_text_time\",\n",
        "    \"accuracy_test\": test_acc2,\n",
        "    \"macro_f1_test\": test_f12,\n",
        "    \"notes\": \"Text + time features\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation**: Compare TF-IDF text-only vs text+time. If the text+time model shows higher macro F1, time-of-day features add signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) DistilBERT 3-class classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LEN_BERT)\n",
        "\n",
        "train_ds = Dataset.from_pandas(df_train[[\"text\", \"label\"]])\n",
        "val_ds = Dataset.from_pandas(df_val[[\"text\", \"label\"]])\n",
        "test_ds = Dataset.from_pandas(df_test[[\"text\", \"label\"]])\n",
        "\n",
        "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
        "val_ds = val_ds.map(tokenize_fn, batched=True)\n",
        "test_ds = test_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "bert_args = TrainingArguments(\n",
        "    output_dir=os.path.join(MODELS_DIR, \"distilbert\"),\n",
        "    num_train_epochs=BERT_EPOCHS,\n",
        "    per_device_train_batch_size=BERT_BATCH,\n",
        "    per_device_eval_batch_size=BERT_BATCH,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    seed=STUDENT_SEED,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=bert_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "test_metrics = trainer.evaluate(test_ds)\n",
        "\n",
        "preds = trainer.predict(test_ds).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "\n",
        "cm_bert = confusion_matrix(y_test, pred_labels)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_bert, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "            yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "plt.title(\"Confusion Matrix - DistilBERT\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"cm_distilbert.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "metrics_rows.append({\n",
        "    \"model\": \"DistilBERT\",\n",
        "    \"accuracy_test\": test_metrics[\"eval_accuracy\"],\n",
        "    \"macro_f1_test\": test_metrics[\"eval_macro_f1\"],\n",
        "    \"notes\": \"Transformer classifier\"\n",
        "})\n",
        "\n",
        "model.save_pretrained(os.path.join(MODELS_DIR, \"distilbert\"))\n",
        "tokenizer.save_pretrained(os.path.join(MODELS_DIR, \"distilbert\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) (Removed) Unsloth SFT\n",
        "\n",
        "Unsloth has been excluded from this notebook per project requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unsloth has been removed from this notebook.\n",
        "print(\"Unsloth section removed per requirement.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Final comparison + report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "print(metrics_df)\n",
        "\n",
        "# Save metrics\n",
        "metrics_df.to_csv(os.path.join(OUTPUT_DIR, \"metrics.csv\"), index=False)\n",
        "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics_rows, f, indent=2)\n",
        "\n",
        "# Save prediction examples (use DistilBERT if available)\n",
        "if \"pred_labels\" in locals():\n",
        "    pred_examples = pd.DataFrame({\n",
        "        \"text\": df_test[\"text\"].values,\n",
        "        \"gold_label\": df_test[\"label\"].map(label_map),\n",
        "        \"pred_label\": [label_map[int(p)] for p in pred_labels],\n",
        "    })\n",
        "    pred_examples.to_csv(os.path.join(TABLES_DIR, \"pred_examples.csv\"), index=False)\n",
        "    print(\"✓ Saved prediction examples to outputs/tables/pred_examples.csv\")\n",
        "\n",
        "# Report insights\n",
        "max_hour = int(neg_by_hour.loc[neg_by_hour[\"label\"].idxmax(), \"hour\"])\n",
        "max_daypart = neg_by_daypart.loc[neg_by_daypart[\"label\"].idxmax(), \"daypart\"]\n",
        "\n",
        "m1 = metrics_df[metrics_df[\"model\"] == \"TFIDF_text_only\"][\"macro_f1_test\"].values\n",
        "m2 = metrics_df[metrics_df[\"model\"] == \"TFIDF_text_time\"][\"macro_f1_test\"].values\n",
        "if len(m1) and len(m2):\n",
        "    diff = m2[0] - m1[0]\n",
        "else:\n",
        "    diff = 0.0\n",
        "\n",
        "final_report = f\"\"\"\n",
        "# Week 7 Final Report\n",
        "\n",
        "## RQ1: Does time-of-day relate to negativity?\n",
        "- Highest negativity by hour (train): hour={max_hour}\n",
        "- Highest negativity by daypart (train): {max_daypart}\n",
        "- Rolling negativity plot shows temporal drift patterns over time.\n",
        "\n",
        "## RQ2: Do time features improve prediction vs text-only?\n",
        "- TFIDF_text_only macro F1: {m1[0] if len(m1) else 'N/A'}\n",
        "- TFIDF_text_time macro F1: {m2[0] if len(m2) else 'N/A'}\n",
        "- Difference (time - text): {diff}\n",
        "\n",
        "## Compute tradeoffs\n",
        "- TF-IDF models are fast and CPU-friendly.\n",
        "- DistilBERT improves language understanding but is slower.\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"final_report.md\"), \"w\") as f:\n",
        "    f.write(final_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Zip outputs + download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "zip_path = shutil.make_archive(\"outputs_week7\", \"zip\", OUTPUT_DIR)\n",
        "print(\"Created:\", zip_path)\n",
        "print(\"Size (MB):\", os.path.getsize(zip_path) / (1024 * 1024))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download in Colab\n",
        "from google.colab import files\n",
        "files.download(\"outputs_week7.zip\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
