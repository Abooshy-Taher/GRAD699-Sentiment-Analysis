{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 5: Unsloth Fine-tuning for Sentiment Classification\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook fine-tunes a small LLaMA-family model using **Unsloth** for ternary sentiment classification (Negative/Neutral/Positive) on Amazon product reviews.\n",
        "\n",
        "### Key Features:\n",
        "- ✅ **Unsloth-based fine-tuning** with LoRA + 4-bit quantization\n",
        "- ✅ **Chronological split** (same as Week 4) to prevent data leakage\n",
        "- ✅ **Instruction-style SFT format** for LLM training\n",
        "- ✅ **Colab-ready** with GPU checks and flexible data loading\n",
        "- ✅ **Reproducible** with random seeds and split summaries\n",
        "\n",
        "### Dataset:\n",
        "- **Source**: Amazon_Data.csv\n",
        "- **Target**: Sentiment labels derived from ratings:\n",
        "  - Rating ≤ 2 → Negative (label 0)\n",
        "  - Rating = 3 → Neutral (label 1)\n",
        "  - Rating ≥ 4 → Positive (label 2)\n",
        "\n",
        "### Split Strategy:\n",
        "- **70% Train** (oldest data)\n",
        "- **15% Validation** (middle)\n",
        "- **15% Test** (most recent data)\n",
        "\n",
        "**⚠️ CRITICAL**: No shuffling before split - maintains strict chronological order to prevent temporal leakage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Install required packages for Unsloth fine-tuning. This cell should be run first in Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth and dependencies\n",
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
        "%pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" \"peft<0.10.0\" \"accelerate<0.30.0\" \"bitsandbytes<0.43.0\" --quiet\n",
        "%pip install \"datasets>=2.18.0\" \"transformers>=4.40.0\" \"evaluate>=0.4.1\" \"scikit-learn>=1.3.0\" --quiet\n",
        "\n",
        "print(\"✓ Dependencies installed successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. GPU Check\n",
        "\n",
        "Verify GPU availability (required for Unsloth fine-tuning).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"⚠️  WARNING: No GPU detected. Unsloth fine-tuning requires a GPU.\")\n",
        "    print(\"   Please enable GPU in Colab: Runtime → Change runtime type → GPU (T4 or A100)\")\n",
        "    raise RuntimeError(\"GPU required for Unsloth fine-tuning\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Imports and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import utility functions\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "try:\n",
        "    from week5_utils import (\n",
        "        load_dataset, clean_data, create_sentiment_labels,\n",
        "        chronological_split, label_to_string, string_to_label\n",
        "    )\n",
        "    USE_UTILS = True\n",
        "except ImportError:\n",
        "    # If running in Colab, define functions inline\n",
        "    print(\"⚠️  week5_utils.py not found. Defining functions inline...\")\n",
        "    USE_UTILS = False\n",
        "    \n",
        "    # Define utility functions inline for Colab\n",
        "    def load_dataset():\n",
        "        possible_paths = [\n",
        "            \"/content/drive/MyDrive/Amazon_Data.csv\",\n",
        "            \"/content/Amazon_Data.csv\",\n",
        "            \"Amazon_Data.csv\",\n",
        "        ]\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                df = pd.read_csv(path)\n",
        "                print(f\"✓ Found file at: {path}\")\n",
        "                return df\n",
        "        raise FileNotFoundError(\"Could not find Amazon_Data.csv. Please upload to /content/drive/MyDrive/\")\n",
        "    \n",
        "    def clean_data(df):\n",
        "        df = df[['text', 'rating', 'timestamp']].copy()\n",
        "        df = df.dropna()\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "        df = df.dropna(subset=['timestamp'])\n",
        "        df = df[df['text'].astype(str).str.len() > 0].copy()\n",
        "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "        return df\n",
        "    \n",
        "    def create_sentiment_labels(df):\n",
        "        df = df.copy()\n",
        "        df['sentiment_label'] = df['rating'].apply(lambda r: 0 if r <= 2 else (1 if r == 3 else 2))\n",
        "        return df\n",
        "    \n",
        "    def chronological_split(df, train_ratio=0.70, val_ratio=0.15):\n",
        "        n_total = len(df)\n",
        "        n_train = int(train_ratio * n_total)\n",
        "        n_val = int(val_ratio * n_total)\n",
        "        df_train = df.iloc[:n_train].copy()\n",
        "        df_val = df.iloc[n_train:n_train + n_val].copy()\n",
        "        df_test = df.iloc[n_train + n_val:].copy()\n",
        "        y_train = df_train['sentiment_label'].values\n",
        "        y_val = df_val['sentiment_label'].values\n",
        "        y_test = df_test['sentiment_label'].values\n",
        "        return df_train, df_val, df_test, y_train, y_val, y_test\n",
        "    \n",
        "    def label_to_string(label):\n",
        "        return {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}[label]\n",
        "    \n",
        "    def string_to_label(label_str):\n",
        "        label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "        label_str_lower = label_str.strip().lower()\n",
        "        for key, value in label_map.items():\n",
        "            if key.lower() in label_str_lower or label_str_lower in key.lower():\n",
        "                return value\n",
        "        return 1\n",
        "\n",
        "# Unsloth imports\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Evaluation imports\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "torch.cuda.manual_seed_all(RANDOM_STATE)\n",
        "\n",
        "print(\"✓ Imports complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load and Prepare Data\n",
        "\n",
        "Load the dataset and apply the same chronological split logic as Week 4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "df = load_dataset()\n",
        "\n",
        "# Clean data\n",
        "print(\"\\nCleaning data...\")\n",
        "df = clean_data(df)\n",
        "\n",
        "# Create sentiment labels from ratings\n",
        "print(\"\\nCreating sentiment labels...\")\n",
        "df = create_sentiment_labels(df)\n",
        "\n",
        "# Chronological split (70/15/15)\n",
        "print(\"\\nPerforming chronological split...\")\n",
        "df_train, df_val, df_test, y_train, y_val, y_test = chronological_split(df)\n",
        "\n",
        "print(f\"\\n✓ Data preparation complete\")\n",
        "print(f\"  Train: {len(df_train):,} samples\")\n",
        "print(f\"  Val:   {len(df_val):,} samples\")\n",
        "print(f\"  Test:  {len(df_test):,} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare Dataset for Unsloth (Instruction Format)\n",
        "\n",
        "Convert classification task to instruction-following format for LLM fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_instruction_prompt(text):\n",
        "    \"\"\"\n",
        "    Create instruction prompt for sentiment classification.\n",
        "    \n",
        "    Args:\n",
        "        text: Review text\n",
        "        \n",
        "    Returns:\n",
        "        str: Formatted instruction prompt\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Classify the sentiment of this review as one of: Negative, Neutral, Positive.\n",
        "\n",
        "Review: {text}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def prepare_dataset_for_unsloth(df_split, y_split):\n",
        "    \"\"\"\n",
        "    Prepare dataset in Hugging Face format with instruction prompts.\n",
        "    \n",
        "    Args:\n",
        "        df_split: Dataframe with 'text' column\n",
        "        y_split: Array of numeric labels (0, 1, 2)\n",
        "        \n",
        "    Returns:\n",
        "        Dataset: Hugging Face dataset with 'text' and 'label' fields\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    labels = []\n",
        "    \n",
        "    for idx, row in df_split.iterrows():\n",
        "        # Create instruction prompt\n",
        "        instruction = create_instruction_prompt(row['text'])\n",
        "        texts.append(instruction)\n",
        "        \n",
        "        # Convert numeric label to string\n",
        "        label_str = label_to_string(y_split[idx])\n",
        "        labels.append(label_str)\n",
        "    \n",
        "    # Create Hugging Face dataset\n",
        "    dataset_dict = {\n",
        "        'text': texts,\n",
        "        'label': labels\n",
        "    }\n",
        "    \n",
        "    dataset = Dataset.from_dict(dataset_dict)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Prepare datasets\n",
        "print(\"Preparing training dataset...\")\n",
        "train_dataset = prepare_dataset_for_unsloth(df_train, y_train)\n",
        "\n",
        "print(\"Preparing validation dataset...\")\n",
        "val_dataset = prepare_dataset_for_unsloth(df_val, y_val)\n",
        "\n",
        "print(\"Preparing test dataset...\")\n",
        "test_dataset = prepare_dataset_for_unsloth(df_test, y_test)\n",
        "\n",
        "print(f\"\\n✓ Datasets prepared:\")\n",
        "print(f\"  Train: {len(train_dataset):,} examples\")\n",
        "print(f\"  Val:   {len(val_dataset):,} examples\")\n",
        "print(f\"  Test:  {len(test_dataset):,} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Model with Unsloth\n",
        "\n",
        "Load a small LLaMA-family model with 4-bit quantization and LoRA adapters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "# Using Llama-3.1-8B-Instruct for good balance of performance and speed\n",
        "# Alternatives: \"unsloth/llama-3.1-8b-bnb-4bit\" or \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
        "model_name = \"unsloth/llama-3.1-8b-bnb-4bit\"\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "print(\"This may take a few minutes on first run...\")\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=512,  # Adjust based on your review length\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=True,  # 4-bit quantization\n",
        ")\n",
        "\n",
        "# Add LoRA adapters for efficient fine-tuning\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank (higher = more capacity, but slower)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,  # LoRA alpha scaling\n",
        "    lora_dropout=0.1,  # Dropout for LoRA\n",
        "    bias=\"none\",  # No bias\n",
        "    use_gradient_checkpointing=True,  # Save memory\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "print(\"✓ Model loaded successfully\")\n",
        "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Configure Tokenizer and Format Dataset\n",
        "\n",
        "Set up tokenizer for instruction-following format and prepare datasets for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure tokenizer\n",
        "tokenizer = FastLanguageModel.get_peft_tokenizer(model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Format dataset for training (instruction + completion)\n",
        "def format_dataset(examples):\n",
        "    \"\"\"\n",
        "    Format dataset for instruction-following training.\n",
        "    Each example has 'text' (instruction) and 'label' (completion).\n",
        "    \"\"\"\n",
        "    inputs = examples['text']\n",
        "    outputs = examples['label']\n",
        "    \n",
        "    # Combine instruction and completion\n",
        "    texts = [f\"{inp}{out}\" for inp, out in zip(inputs, outputs)]\n",
        "    \n",
        "    # Tokenize\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=False,\n",
        "    )\n",
        "    \n",
        "    # For causal LM, labels are the same as input_ids\n",
        "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# Apply formatting\n",
        "print(\"Formatting training dataset...\")\n",
        "train_dataset_formatted = train_dataset.map(\n",
        "    format_dataset,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "\n",
        "print(\"Formatting validation dataset...\")\n",
        "val_dataset_formatted = val_dataset.map(\n",
        "    format_dataset,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        ")\n",
        "\n",
        "print(\"✓ Datasets formatted for training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Configuration\n",
        "\n",
        "Set up training arguments for fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,  # Small batch size for memory efficiency\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,  # Start with 1 epoch (can increase to 2-3 if needed)\n",
        "    learning_rate=2e-4,  # Learning rate for LoRA\n",
        "    fp16=not torch.cuda.is_bf16_supported(),  # Use fp16 if bf16 not supported\n",
        "    bf16=torch.cuda.is_bf16_supported(),  # Use bf16 if supported (A100)\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,  # Evaluate every 500 steps\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    output_dir=\"./outputs\",\n",
        "    optim=\"adamw_8bit\",  # 8-bit optimizer to save memory\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard\n",
        "    seed=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "print(\"✓ Training arguments configured\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Total steps: ~{len(train_dataset_formatted) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Fine-tune Model\n",
        "\n",
        "Train the model using SFTTrainer from Unsloth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset_formatted,\n",
        "    eval_dataset=val_dataset_formatted,\n",
        "    args=training_args,\n",
        "    dataset_text_field=\"text\",  # Not used since we pre-formatted, but required\n",
        "    max_seq_length=512,\n",
        "    packing=False,  # Don't pack sequences\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer created\")\n",
        "print(\"\\nStarting training...\")\n",
        "print(\"This may take 30-60 minutes depending on GPU and dataset size.\")\n",
        "\n",
        "# Train\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n✓ Training complete!\")\n",
        "print(f\"  Training loss: {trainer_stats.training_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Model\n",
        "\n",
        "Save the fine-tuned model for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "model.save_pretrained(\"unsloth_sentiment_model\")\n",
        "tokenizer.save_pretrained(\"unsloth_sentiment_model\")\n",
        "\n",
        "print(\"✓ Model saved to 'unsloth_sentiment_model/'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Inference Function\n",
        "\n",
        "Create a function to run inference on new examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "def predict_sentiment(text, model=model, tokenizer=tokenizer):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a single review text.\n",
        "    \n",
        "    Args:\n",
        "        text: Review text\n",
        "        model: Fine-tuned model\n",
        "        tokenizer: Tokenizer\n",
        "        \n",
        "    Returns:\n",
        "        str: Predicted label (\"Negative\", \"Neutral\", or \"Positive\")\n",
        "    \"\"\"\n",
        "    # Create instruction prompt\n",
        "    prompt = create_instruction_prompt(text)\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        [prompt],\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    # Generate\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=5,  # Short generation (just the label)\n",
        "        temperature=0.0,  # Deterministic\n",
        "        do_sample=False,  # Greedy decoding\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    \n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract answer (everything after \"Answer:\")\n",
        "    if \"Answer:\" in generated_text:\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "    else:\n",
        "        answer = generated_text.strip()\n",
        "    \n",
        "    # Parse label (take first word, handle case variations)\n",
        "    answer_words = answer.split()\n",
        "    if len(answer_words) > 0:\n",
        "        predicted_label = answer_words[0].strip()\n",
        "    else:\n",
        "        predicted_label = answer.strip()\n",
        "    \n",
        "    # Normalize to one of our labels\n",
        "    predicted_label = string_to_label(predicted_label)\n",
        "    return label_to_string(predicted_label)\n",
        "\n",
        "\n",
        "# Test on a few examples\n",
        "print(\"Testing inference on sample reviews:\")\n",
        "print(\"=\" * 60)\n",
        "for i in range(3):\n",
        "    sample_text = df_test.iloc[i]['text'][:200] + \"...\"\n",
        "    true_label = label_to_string(y_test[i])\n",
        "    pred_label = predict_sentiment(df_test.iloc[i]['text'])\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"  Text: {sample_text}\")\n",
        "    print(f\"  True: {true_label}\")\n",
        "    print(f\"  Pred: {pred_label}\")\n",
        "    print(f\"  Match: {'✓' if true_label == pred_label else '✗'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Evaluate on Validation Set\n",
        "\n",
        "Evaluate model performance on validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set (sample for speed)\n",
        "print(\"Evaluating on validation set...\")\n",
        "print(\"(Using a sample for faster evaluation - adjust sample_size as needed)\")\n",
        "\n",
        "sample_size = min(1000, len(df_val))  # Evaluate on 1000 samples\n",
        "val_sample_idx = np.random.choice(len(df_val), sample_size, replace=False)\n",
        "val_texts_sample = df_val.iloc[val_sample_idx]['text'].values\n",
        "y_val_true_sample = y_val[val_sample_idx]\n",
        "\n",
        "# Predict\n",
        "print(\"Running predictions...\")\n",
        "y_val_pred_sample = []\n",
        "for text in val_texts_sample:\n",
        "    pred = predict_sentiment(text)\n",
        "    y_val_pred_sample.append(pred)\n",
        "\n",
        "# Convert to numeric labels\n",
        "y_val_pred_numeric = np.array([string_to_label(pred) for pred in y_val_pred_sample])\n",
        "y_val_true_numeric = np.array([string_to_label(label_to_string(label)) for label in y_val_true_sample])\n",
        "\n",
        "# Metrics\n",
        "val_acc = accuracy_score(y_val_true_numeric, y_val_pred_numeric)\n",
        "print(f\"\\n✓ Validation Evaluation Complete\")\n",
        "print(f\"  Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_val_true_numeric, y_val_pred_numeric, \n",
        "                          target_names=['Negative', 'Neutral', 'Positive']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Final Evaluation on Test Set\n",
        "\n",
        "**⚠️ CRITICAL**: Test set is used ONLY ONCE for final evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final evaluation on test set\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL EVALUATION ON TEST SET\")\n",
        "print(\"=\" * 60)\n",
        "print(\"⚠️  This is the FIRST and ONLY time the test set is used\")\n",
        "print(\"⚠️  Model selected based on validation performance only\\n\")\n",
        "\n",
        "# For full evaluation, use entire test set (may take time)\n",
        "# For faster evaluation, use a sample\n",
        "FULL_TEST_EVAL = False  # Set to True for full test set evaluation\n",
        "\n",
        "if FULL_TEST_EVAL:\n",
        "    test_texts = df_test['text'].values\n",
        "    y_test_true = y_test\n",
        "    print(f\"Evaluating on full test set ({len(test_texts):,} samples)...\")\n",
        "else:\n",
        "    # Sample for faster evaluation\n",
        "    test_sample_size = min(2000, len(df_test))\n",
        "    test_sample_idx = np.random.choice(len(df_test), test_sample_size, replace=False)\n",
        "    test_texts = df_test.iloc[test_sample_idx]['text'].values\n",
        "    y_test_true = y_test[test_sample_idx]\n",
        "    print(f\"Evaluating on test set sample ({len(test_texts):,} samples)...\")\n",
        "    print(\"(Set FULL_TEST_EVAL=True for full evaluation)\")\n",
        "\n",
        "# Predict\n",
        "print(\"Running predictions...\")\n",
        "y_test_pred = []\n",
        "for i, text in enumerate(test_texts):\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"  Processed {i+1}/{len(test_texts)} samples...\")\n",
        "    pred = predict_sentiment(text)\n",
        "    y_test_pred.append(pred)\n",
        "\n",
        "# Convert to numeric labels\n",
        "y_test_pred_numeric = np.array([string_to_label(pred) for pred in y_test_pred])\n",
        "y_test_true_numeric = np.array([string_to_label(label_to_string(label)) for label in y_test_true])\n",
        "\n",
        "# Metrics\n",
        "test_acc = accuracy_score(y_test_true_numeric, y_test_pred_numeric)\n",
        "print(f\"\\n✓ Test Evaluation Complete\")\n",
        "print(f\"  Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_true_numeric, y_test_pred_numeric, \n",
        "                          target_names=['Negative', 'Neutral', 'Positive']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_true_numeric, y_test_pred_numeric)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.title('Confusion Matrix - Test Set (Unsloth Fine-tuned Model)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Example Predictions\n",
        "\n",
        "Show some example predictions vs true labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show example predictions\n",
        "print(\"Example Predictions:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "n_examples = 10\n",
        "for i in range(min(n_examples, len(test_texts))):\n",
        "    true_label = label_to_string(y_test_true_numeric[i])\n",
        "    pred_label = y_test_pred[i]\n",
        "    match = \"✓\" if true_label == pred_label else \"✗\"\n",
        "    \n",
        "    print(f\"\\nExample {i+1} [{match}]\")\n",
        "    print(f\"  Text: {test_texts[i][:150]}...\")\n",
        "    print(f\"  True Label: {true_label}\")\n",
        "    print(f\"  Predicted:  {pred_label}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Summary\n",
        "\n",
        "### Key Results:\n",
        "- **Model**: Llama-3.1-8B fine-tuned with Unsloth (LoRA + 4-bit)\n",
        "- **Training**: Instruction-following format with chronological split\n",
        "- **Test Accuracy**: See results above\n",
        "\n",
        "### Reproducibility:\n",
        "- ✅ Random seeds set (RANDOM_STATE=42)\n",
        "- ✅ Chronological split (no shuffling)\n",
        "- ✅ Same label mapping as Week 4 (0=Negative, 1=Neutral, 2=Positive)\n",
        "\n",
        "### Data Leakage Prevention:\n",
        "- ✅ Chronological split by timestamp\n",
        "- ✅ No shuffling before split\n",
        "- ✅ Test set used only once\n",
        "\n",
        "### Next Steps:\n",
        "- Increase epochs (2-3) for better performance\n",
        "- Tune LoRA rank (r) and learning rate\n",
        "- Try different base models (Qwen2.5, Mistral)\n",
        "- Full test set evaluation (set FULL_TEST_EVAL=True)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
