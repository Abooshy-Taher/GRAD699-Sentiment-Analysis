{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 8 — Final Sentiment Analysis Model (Binary)\n",
        "\n",
        "**Thesis Context**: This notebook reproduces and improves the end‑to‑end experiment for sentiment analysis of Amazon reviews, with a specific focus on **time‑of‑day and negativity**.\n",
        "\n",
        "**Research Questions**:\n",
        "- **RQ1**: Does time‑of‑day relate to negativity patterns in reviews?\n",
        "- **RQ2**: Do engineered time‑based features improve sentiment prediction beyond text‑only models?\n",
        "\n",
        "**Classification (Binary)**:\n",
        "- Negative (0): rating ≤ 2\n",
        "- Positive (1): rating ≥ 3\n",
        "\n",
        "**Seed Requirement**:\n",
        "This notebook runs the full pipeline **twice**:\n",
        "- Run A: seed = 319302\n",
        "- Run B: random 6‑digit seed generated at runtime\n",
        "\n",
        "**Note**: This notebook **does not use Unsloth**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Colab Setup (optional Google Drive)\n",
        "If you want to store outputs in Google Drive, uncomment and run the cell below, then set `USE_DRIVE = True` in the configuration cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1) Install + Imports (single cell) ===\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "# Disable W&B by default\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Install required packages (single clean cell)\n",
        "packages = [\n",
        "    \"pandas>=2.0.0\",\n",
        "    \"numpy>=1.24.0\",\n",
        "    \"matplotlib>=3.7.0\",\n",
        "    \"seaborn>=0.12.0\",\n",
        "    \"scikit-learn>=1.3.0\",\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"datasets>=2.18.0\",\n",
        "    \"evaluate>=0.4.1\",\n",
        "    \"accelerate>=0.20.0\",\n",
        "    \"pyarrow>=10.0.0\",\n",
        "]\n",
        "for pkg in packages:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# GPU check\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"⚠️  No GPU detected. Transformer will run in FAST_RUN mode.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2) Configuration ===\n",
        "DATA_PATH = \"/content/Amazon_Data.csv\"  # set your path\n",
        "FILE_TYPE = \"auto\"  # \"csv\", \"parquet\", \"jsonl\", or \"auto\"\n",
        "TEXT_COL = None     # set if your column names differ\n",
        "RATING_COL = None   # set if your column names differ\n",
        "TIME_COL = None     # set if your column names differ\n",
        "\n",
        "STUDENT_SEED = 319302\n",
        "FAST_RUN = True\n",
        "SAMPLE_FOR_TEXT = 100000  # for heavy text models; set None to use full train\n",
        "\n",
        "MAX_SEQ_LEN_BERT = 256\n",
        "BERT_EPOCHS = 1 if FAST_RUN else 3\n",
        "BERT_BATCH = 16\n",
        "\n",
        "USE_DRIVE = False\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/GRAD699/Week8/\"\n",
        "\n",
        "# Output folders\n",
        "OUTPUT_DIR = DRIVE_OUTPUT_DIR if USE_DRIVE else \"outputs\"\n",
        "FIGURES_DIR = os.path.join(OUTPUT_DIR, \"figures\")\n",
        "MODELS_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
        "TABLES_DIR = os.path.join(OUTPUT_DIR, \"tables\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(TABLES_DIR, exist_ok=True)\n",
        "\n",
        "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
        "\n",
        "print(\"✓ Config loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_global_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def parse_timestamp(series: pd.Series) -> pd.Series:\n",
        "    if pd.api.types.is_numeric_dtype(series):\n",
        "        max_val = series.max()\n",
        "        if max_val > 1e12:\n",
        "            return pd.to_datetime(series, errors=\"coerce\", unit=\"ms\")\n",
        "        if max_val > 1e9:\n",
        "            return pd.to_datetime(series, errors=\"coerce\", unit=\"s\")\n",
        "    return pd.to_datetime(series, errors=\"coerce\")\n",
        "\n",
        "def daypart_from_hour(hour: int) -> str:\n",
        "    if 0 <= hour <= 4:\n",
        "        return \"late_night\"\n",
        "    if 5 <= hour <= 11:\n",
        "        return \"morning\"\n",
        "    if 12 <= hour <= 16:\n",
        "        return \"afternoon\"\n",
        "    if 17 <= hour <= 20:\n",
        "        return \"evening\"\n",
        "    return \"night\"\n",
        "\n",
        "def load_data(path: str, file_type: str) -> pd.DataFrame:\n",
        "    ftype = file_type.lower() if isinstance(file_type, str) else \"auto\"\n",
        "    if ftype == \"auto\":\n",
        "        if path.endswith(\".parquet\"):\n",
        "            ftype = \"parquet\"\n",
        "        elif path.endswith(\".jsonl\"):\n",
        "            ftype = \"jsonl\"\n",
        "        else:\n",
        "            ftype = \"csv\"\n",
        "\n",
        "    if ftype == \"csv\":\n",
        "        return pd.read_csv(path)\n",
        "    if ftype == \"parquet\":\n",
        "        return pd.read_parquet(path)\n",
        "    if ftype == \"jsonl\":\n",
        "        return pd.read_json(path, lines=True)\n",
        "    raise ValueError(\"FILE_TYPE must be one of: csv, parquet, jsonl, auto\")\n",
        "\n",
        "def find_column(df, candidates, override=None):\n",
        "    if override:\n",
        "        if override in df.columns:\n",
        "            return override\n",
        "        raise ValueError(f\"Column '{override}' not found in dataset.\")\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def chronological_split(df, train_ratio=0.8, val_ratio=0.1):\n",
        "    n = len(df)\n",
        "    n_train = int(train_ratio * n)\n",
        "    n_val = int(val_ratio * n)\n",
        "    train = df.iloc[:n_train].copy()\n",
        "    val = df.iloc[n_train:n_train + n_val].copy()\n",
        "    test = df.iloc[n_train + n_val:].copy()\n",
        "    return train, val, test\n",
        "\n",
        "def eval_binary_metrics(y_true, y_pred, y_proba=None):\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "    }\n",
        "    if y_proba is not None:\n",
        "        try:\n",
        "            metrics[\"roc_auc\"] = roc_auc_score(y_true, y_proba)\n",
        "        except Exception:\n",
        "            metrics[\"roc_auc\"] = float(\"nan\")\n",
        "    else:\n",
        "        metrics[\"roc_auc\"] = float(\"nan\")\n",
        "    return metrics\n",
        "\n",
        "def build_time_features(df):\n",
        "    df = df.copy()\n",
        "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "    df[\"day_of_week\"] = df[\"timestamp\"].dt.dayofweek\n",
        "    df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
        "    df[\"daypart\"] = df[\"hour\"].apply(daypart_from_hour)\n",
        "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
        "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
        "    return df\n",
        "\n",
        "def build_time_matrix(train_df, val_df, test_df):\n",
        "    num_features = [\"hour_sin\", \"hour_cos\", \"is_weekend\"]\n",
        "    cat_features = [\"day_of_week\", \"daypart\"]\n",
        "    enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
        "    enc.fit(train_df[cat_features])\n",
        "    train_cat = enc.transform(train_df[cat_features])\n",
        "    val_cat = enc.transform(val_df[cat_features])\n",
        "    test_cat = enc.transform(test_df[cat_features])\n",
        "    train_num = csr_matrix(train_df[num_features].values)\n",
        "    val_num = csr_matrix(val_df[num_features].values)\n",
        "    test_num = csr_matrix(test_df[num_features].values)\n",
        "    return (hstack([train_num, train_cat]),\n",
        "            hstack([val_num, val_cat]),\n",
        "            hstack([test_num, test_cat]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Load data (raw showcase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw = load_data(DATA_PATH, FILE_TYPE)\n",
        "\n",
        "text_col = find_column(df_raw, [\"text\", \"review\", \"reviewText\"], TEXT_COL)\n",
        "rating_col = find_column(df_raw, [\"rating\", \"stars\", \"overall\"], RATING_COL)\n",
        "time_col = find_column(df_raw, [\"timestamp\", \"reviewTime\", \"time\"], TIME_COL)\n",
        "\n",
        "if text_col is None or rating_col is None or time_col is None:\n",
        "    raise ValueError(\"Missing required columns. Please set TEXT_COL, RATING_COL, TIME_COL.\")\n",
        "\n",
        "df_raw = df_raw.rename(columns={text_col: \"text\", rating_col: \"rating\", time_col: \"timestamp\"})\n",
        "\n",
        "print(\"Shape:\", df_raw.shape)\n",
        "print(\"Columns:\", df_raw.columns.tolist())\n",
        "print(\"Dtypes:\")\n",
        "print(df_raw.dtypes)\n",
        "\n",
        "print(\"\\nSample rows:\")\n",
        "display(df_raw[[\"timestamp\", \"rating\", \"text\"]].head(20))\n",
        "\n",
        "print(\"\\nMissingness summary:\")\n",
        "print(df_raw[[\"text\", \"rating\", \"timestamp\"]].isna().sum())\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "df_raw[\"rating\"].value_counts().sort_index().plot(kind=\"bar\")\n",
        "plt.title(\"Raw Rating Distribution\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"raw_rating_distribution.png\"), dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Clean + preprocess + label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df_raw.dropna(subset=[\"text\", \"rating\", \"timestamp\"]).copy()\n",
        "df[\"timestamp\"] = parse_timestamp(df[\"timestamp\"])\n",
        "df = df.dropna(subset=[\"timestamp\"]).copy()\n",
        "df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
        "df = df[df[\"text\"].str.len() > 0].copy()\n",
        "\n",
        "# Binary label mapping\n",
        "df[\"label\"] = df[\"rating\"].apply(lambda r: 0 if r <= 2 else 1)\n",
        "df[\"label_name\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "# Basic text features\n",
        "df[\"review_len\"] = df[\"text\"].str.len()\n",
        "df[\"word_count\"] = df[\"text\"].str.split().str.len()\n",
        "\n",
        "# Sort chronologically\n",
        "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "print(\"Cleaned shape:\", df.shape)\n",
        "print(df[[\"label_name\"]].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Feature engineering (time-of-day)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = build_time_features(df)\n",
        "print(\"✓ Time features created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Chronological split (train/val/test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train, df_val, df_test = chronological_split(df, train_ratio=0.8, val_ratio=0.1)\n",
        "\n",
        "print(\"Train range:\", df_train[\"timestamp\"].min(), \"to\", df_train[\"timestamp\"].max())\n",
        "print(\"Val range:\", df_val[\"timestamp\"].min(), \"to\", df_val[\"timestamp\"].max())\n",
        "print(\"Test range:\", df_test[\"timestamp\"].min(), \"to\", df_test[\"timestamp\"].max())\n",
        "\n",
        "assert df_train[\"timestamp\"].max() <= df_val[\"timestamp\"].min(), \"Train/Val overlap\"\n",
        "assert df_val[\"timestamp\"].max() <= df_test[\"timestamp\"].min(), \"Val/Test overlap\"\n",
        "\n",
        "print(\"Sizes:\", len(df_train), len(df_val), len(df_test))\n",
        "print(\"Train label distribution:\\n\", df_train[\"label_name\"].value_counts())\n",
        "print(\"Val label distribution:\\n\", df_val[\"label_name\"].value_counts())\n",
        "print(\"Test label distribution:\\n\", df_test[\"label_name\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) RQ1 Visualizations (negativity vs time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Negativity rate by hour\n",
        "neg_by_hour = df.groupby(\"hour\")[\"label\"].apply(lambda x: (x == 0).mean()).reset_index(name=\"neg_rate\")\n",
        "vol_by_hour = df.groupby(\"hour\").size().reset_index(name=\"count\")\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(neg_by_hour[\"hour\"], neg_by_hour[\"neg_rate\"], marker=\"o\")\n",
        "plt.title(\"Negativity Rate by Hour\")\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"Negativity Rate\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"negativity_by_hour.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar(vol_by_hour[\"hour\"], vol_by_hour[\"count\"], color=\"steelblue\")\n",
        "plt.title(\"Review Volume by Hour\")\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"volume_by_hour.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Negativity rate by daypart\n",
        "neg_by_daypart = df.groupby(\"daypart\")[\"label\"].apply(lambda x: (x == 0).mean()).reset_index(name=\"neg_rate\")\n",
        "vol_by_daypart = df.groupby(\"daypart\").size().reset_index(name=\"count\")\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=\"daypart\", y=\"neg_rate\", data=neg_by_daypart, color=\"salmon\")\n",
        "plt.title(\"Negativity Rate by Daypart\")\n",
        "plt.ylabel(\"Negativity Rate\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"negativity_by_daypart.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=\"daypart\", y=\"count\", data=vol_by_daypart, color=\"steelblue\")\n",
        "plt.title(\"Review Volume by Daypart\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"volume_by_daypart.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Heatmap: negativity rate by hour x day_of_week\n",
        "heat = df.groupby([\"hour\", \"day_of_week\"])[\"label\"].apply(lambda x: (x == 0).mean()).reset_index()\n",
        "heat_pivot = heat.pivot(index=\"hour\", columns=\"day_of_week\", values=\"label\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(heat_pivot, cmap=\"Reds\")\n",
        "plt.title(\"Negativity Rate Heatmap (Hour x Day of Week)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, \"negativity_heatmap.png\"), dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Run the full pipeline twice (Run A + Run B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_b_seed = random.SystemRandom().randint(100000, 999999)\n",
        "print(\"Run A seed:\", STUDENT_SEED)\n",
        "print(\"Run B seed:\", run_b_seed)\n",
        "\n",
        "metrics_rows = []\n",
        "run_summaries = []\n",
        "\n",
        "def run_models(seed: int, run_name: str):\n",
        "    set_global_seed(seed)\n",
        "\n",
        "    # Sampling for text models (train only)\n",
        "    train_text = df_train\n",
        "    if SAMPLE_FOR_TEXT is not None and len(df_train) > SAMPLE_FOR_TEXT:\n",
        "        train_text = df_train.head(SAMPLE_FOR_TEXT).copy()\n",
        "\n",
        "    # Baseline 1: TF-IDF text only\n",
        "    tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=2)\n",
        "    X_train = tfidf.fit_transform(train_text[\"text\"].values)\n",
        "    X_val = tfidf.transform(df_val[\"text\"].values)\n",
        "    X_test = tfidf.transform(df_test[\"text\"].values)\n",
        "\n",
        "    y_train = train_text[\"label\"].values\n",
        "    y_val = df_val[\"label\"].values\n",
        "    y_test = df_test[\"label\"].values\n",
        "\n",
        "    clf_text = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "    clf_text.fit(X_train, y_train)\n",
        "\n",
        "    test_pred = clf_text.predict(X_test)\n",
        "    test_proba = clf_text.predict_proba(X_test)[:, 1]\n",
        "    m1 = eval_binary_metrics(y_test, test_pred, test_proba)\n",
        "\n",
        "    metrics_rows.append({\"run\": run_name, \"model\": \"Baseline_TFIDF_Text\", **m1})\n",
        "\n",
        "    # Baseline 2: TF-IDF + time features\n",
        "    X_train_time, X_val_time, X_test_time = build_time_matrix(train_text, df_val, df_test)\n",
        "    X_train_combined = hstack([X_train, X_train_time])\n",
        "    X_test_combined = hstack([X_test, X_test_time])\n",
        "\n",
        "    clf_time = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "    clf_time.fit(X_train_combined, y_train)\n",
        "\n",
        "    test_pred2 = clf_time.predict(X_test_combined)\n",
        "    test_proba2 = clf_time.predict_proba(X_test_combined)[:, 1]\n",
        "    m2 = eval_binary_metrics(y_test, test_pred2, test_proba2)\n",
        "\n",
        "    metrics_rows.append({\"run\": run_name, \"model\": \"Baseline_TFIDF_Time\", **m2})\n",
        "\n",
        "    # DistilBERT\n",
        "    bert_train = train_text\n",
        "    bert_val = df_val\n",
        "    bert_test = df_test\n",
        "    if FAST_RUN:\n",
        "        bert_train = bert_train.head(min(20000, len(bert_train)))\n",
        "        bert_val = bert_val.head(min(5000, len(bert_val)))\n",
        "        bert_test = bert_test.head(min(5000, len(bert_test)))\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "    def tokenize_fn(batch):\n",
        "        return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LEN_BERT)\n",
        "\n",
        "    train_ds = Dataset.from_pandas(bert_train[[\"text\", \"label\"]])\n",
        "    val_ds = Dataset.from_pandas(bert_val[[\"text\", \"label\"]])\n",
        "    test_ds = Dataset.from_pandas(bert_test[[\"text\", \"label\"]])\n",
        "\n",
        "    train_ds = train_ds.map(tokenize_fn, batched=True)\n",
        "    val_ds = val_ds.map(tokenize_fn, batched=True)\n",
        "    test_ds = test_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
        "        preds = np.argmax(logits, axis=1)\n",
        "        return eval_binary_metrics(labels, preds, probs)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(MODELS_DIR, f\"distilbert_{run_name}\"),\n",
        "        num_train_epochs=BERT_EPOCHS,\n",
        "        per_device_train_batch_size=BERT_BATCH,\n",
        "        per_device_eval_batch_size=BERT_BATCH,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=50,\n",
        "        seed=seed,\n",
        "        data_seed=seed,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    test_output = trainer.predict(test_ds)\n",
        "    logits = test_output.predictions\n",
        "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    m3 = eval_binary_metrics(bert_test[\"label\"].values, preds, probs)\n",
        "\n",
        "    # Confusion matrix for LLM model (DistilBERT)\n",
        "    cm_bert = confusion_matrix(bert_test[\"label\"].values, preds)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm_bert, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=[\"Negative\", \"Positive\"],\n",
        "                yticklabels=[\"Negative\", \"Positive\"])\n",
        "    plt.title(f\"Confusion Matrix - DistilBERT ({run_name})\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIGURES_DIR, f\"cm_distilbert_{run_name}.png\"), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    metrics_rows.append({\"run\": run_name, \"model\": \"DistilBERT\", **m3})\n",
        "\n",
        "    # Save prediction examples from Run A\n",
        "    if run_name == \"Run_A\":\n",
        "        pred_examples = pd.DataFrame({\n",
        "            \"text\": bert_test[\"text\"].values,\n",
        "            \"gold_label\": bert_test[\"label\"].map(label_map),\n",
        "            \"pred_label\": [label_map[int(p)] for p in preds],\n",
        "        })\n",
        "        pred_examples.to_csv(os.path.join(TABLES_DIR, \"pred_examples.csv\"), index=False)\n",
        "\n",
        "    return m1, m2, m3\n",
        "\n",
        "run_models(STUDENT_SEED, \"Run_A\")\n",
        "run_models(run_b_seed, \"Run_B\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Results + Comparison Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "display(metrics_df)\n",
        "\n",
        "# Save metrics\n",
        "metrics_df.to_csv(os.path.join(OUTPUT_DIR, \"metrics.csv\"), index=False)\n",
        "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics_rows, f, indent=2)\n",
        "\n",
        "# Run A vs Run B comparison table\n",
        "comparison = metrics_df.pivot_table(index=\"model\", columns=\"run\", values=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"])\n",
        "display(comparison)\n",
        "\n",
        "# RQ1 aggregates (stable across runs)\n",
        "max_hour = int(neg_by_hour.loc[neg_by_hour[\"neg_rate\"].idxmax(), \"hour\"])\n",
        "max_daypart = neg_by_daypart.loc[neg_by_daypart[\"neg_rate\"].idxmax(), \"daypart\"]\n",
        "late_night_rate = float(neg_by_daypart[neg_by_daypart[\"daypart\"] == \"late_night\"][\"neg_rate\"].values[0])\n",
        "overall_neg_rate = float((df[\"label\"] == 0).mean())\n",
        "pct_diff = (late_night_rate - overall_neg_rate) * 100\n",
        "\n",
        "summary_text = f\"\"\"\n",
        "### RQ1 Summary\n",
        "- Highest negativity hour: {max_hour}\n",
        "- Highest negativity daypart: {max_daypart}\n",
        "- Late‑night negativity rate: {late_night_rate:.3f}\n",
        "- Overall negativity rate: {overall_neg_rate:.3f}\n",
        "- Late‑night vs overall difference: {pct_diff:.2f} percentage points\n",
        "\"\"\"\n",
        "print(summary_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Interpretation (RQ1 + RQ2)\n",
        "\n",
        "**RQ1**: The hour/daypart aggregates are stable across runs because they are computed from the full dataset with deterministic grouping (no randomness). The highest‑negativity hour and daypart are therefore identical in both runs.\n",
        "\n",
        "**RQ2**: The ML models can vary slightly across runs because random initialization, optimization order, and GPU nondeterminism can affect training. Even with a fixed split and identical hyperparameters, the learned parameters can differ between seeds, which can shift accuracy, F1, and ROC‑AUC.\n",
        "\n",
        "Use the comparison tables above to document whether time‑based features improved performance (Baseline 2 vs Baseline 1) and whether DistilBERT outperforms the classical baselines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Publish to GitHub\n",
        "\n",
        "**Option 1 (Recommended: Manual)**\n",
        "```bash\n",
        "git status\n",
        "git add \"Week 8/Final Sentiment Analysis Model.ipynb\"\n",
        "git commit -m \"Add Week 8 final sentiment analysis notebook\"\n",
        "git push\n",
        "```\n",
        "\n",
        "**Option 2 (From Colab)**\n",
        "```bash\n",
        "git clone https://github.com/<your-username>/<your-repo>.git\n",
        "cd <your-repo>\n",
        "cp \"/content/Final Sentiment Analysis Model.ipynb\" .\n",
        "git add \"Final Sentiment Analysis Model.ipynb\"\n",
        "git commit -m \"Add Week 8 final sentiment analysis notebook\"\n",
        "git push https://<YOUR_TOKEN>@github.com/<your-username>/<your-repo>.git\n",
        "```\n",
        "Use a GitHub Personal Access Token (PAT) for authentication. Do not hardcode tokens in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 12) Zip outputs + download ===\n",
        "import shutil\n",
        "zip_path = shutil.make_archive(\"outputs_week8\", \"zip\", OUTPUT_DIR)\n",
        "print(\"Created:\", zip_path)\n",
        "print(\"Size (MB):\", os.path.getsize(zip_path) / (1024 * 1024))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download in Colab\n",
        "from google.colab import files\n",
        "files.download(\"outputs_week8.zip\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
