{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 6: Final Sentiment Analysis Pipeline\n",
        "\n",
        "## End-to-End Thesis-Quality Experiment Pipeline\n",
        "\n",
        "This notebook combines all components into a clean, reproducible, Colab-ready pipeline:\n",
        "- Data loading & cleaning\n",
        "- Chronological splitting (prevents temporal leakage)\n",
        "- Feature engineering (TF-IDF + time features, fit on train only)\n",
        "- Baseline models (TF-IDF + Logistic Regression)\n",
        "- Transformer fine-tuning (DistilBERT)\n",
        "- Unsloth fine-tuning (Llama-3.1-8B with LoRA)\n",
        "- Final evaluation & comparison\n",
        "\n",
        "**Reproducibility**: seed=319302 throughout\n",
        "**Data Leakage Prevention**: Chronological splits, transforms fit on train only\n",
        "**Colab-Ready**: Auto-detects data paths, includes GPU checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A. Environment Setup (Colab installs, GPU check)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages for Colab\n",
        "# Skip if running locally and packages are already installed\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_if_missing(package):\n",
        "    try:\n",
        "        __import__(package.split(\"==\")[0].split(\">=\")[0].split(\"[\")[0])\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
        "\n",
        "# Core ML packages\n",
        "install_if_missing(\"pandas>=2.0.0\")\n",
        "install_if_missing(\"numpy>=1.24.0\")\n",
        "install_if_missing(\"scikit-learn>=1.3.0\")\n",
        "install_if_missing(\"matplotlib>=3.7.0\")\n",
        "install_if_missing(\"seaborn>=0.12.0\")\n",
        "\n",
        "# Transformer packages\n",
        "install_if_missing(\"transformers>=4.40.0\")\n",
        "install_if_missing(\"datasets>=2.18.0\")\n",
        "install_if_missing(\"evaluate>=0.4.1\")\n",
        "install_if_missing(\"accelerate>=0.20.0\")\n",
        "install_if_missing(\"torch>=2.0.0\")\n",
        "\n",
        "print(\"✓ Core packages installed\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "✓ Core packages installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: Mount Google Drive (uncomment if needed)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# print(\"✓ Google Drive mounted\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    DEVICE = \"cuda\"\n",
        "else:\n",
        "    print(\"⚠️  CPU mode (GPU recommended for transformer/unsloth training)\")\n",
        "    DEVICE = \"cpu\"\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "⚠️  CPU mode (GPU recommended for transformer/unsloth training)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. Imports & Global Config (seed=319302, paths, flags)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, f1_score, accuracy_score\n",
        ")\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Transformer imports\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer\n",
        ")\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "RANDOM_STATE = 319302\n",
        "np.random.seed(RANDOM_STATE)\n",
        "random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
        "\n",
        "# FAST RUN toggles\n",
        "SAMPLE_FRAC = 1.0  # Use 1.0 for full dataset, < 1.0 for quick tests\n",
        "MAX_ROWS = None  # None for all rows, or set limit for debug\n",
        "EPOCHS_BERT = 1  # Epochs for DistilBERT fine-tuning\n",
        "EPOCHS_UNSLOTH = 1  # Epochs for Unsloth fine-tuning\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "MODELS_DIR = \"models\"\n",
        "FIGURES_DIR = os.path.join(OUTPUT_DIR, \"figures\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"✓ Imports complete\")\n",
        "print(f\"✓ Random seed: {RANDOM_STATE}\")\n",
        "print(f\"✓ Sample fraction: {SAMPLE_FRAC}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "✓ Imports complete\n",
            "✓ Random seed: 319302\n",
            "✓ Sample fraction: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Load Data (auto-detect paths, show shape/columns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_dataset():\n",
        "    \"\"\"\n",
        "    Load Amazon_Data.csv from multiple possible locations.\n",
        "    Auto-detects Colab paths and local paths.\n",
        "    \"\"\"\n",
        "    possible_paths = [\n",
        "        # Colab paths\n",
        "        \"/content/drive/MyDrive/Amazon_Data.csv\",\n",
        "        \"/content/Amazon_Data.csv\",\n",
        "        # Local paths\n",
        "        \"../Amazon_Data.csv\",\n",
        "        \"Amazon_Data.csv\",\n",
        "        os.path.join(os.path.expanduser(\"~\"), \"Desktop/HU Classes/GRAD699/Sentiment Analysis/Amazon_Data.csv\")\n",
        "    ]\n",
        "    \n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            df = pd.read_csv(path)\n",
        "            print(f\"✓ Found file at: {path}\")\n",
        "            print(f\"Dataset loaded: {len(df):,} rows, {len(df.columns)} columns\")\n",
        "            print(f\"Columns: {list(df.columns)}\")\n",
        "            return df\n",
        "    \n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find Amazon_Data.csv in any expected location.\\n\"\n",
        "        \"Please place Amazon_Data.csv in one of:\\n\"\n",
        "        \"  - /content/drive/MyDrive/Amazon_Data.csv (Google Drive)\\n\"\n",
        "        \"  - /content/Amazon_Data.csv (Colab upload)\\n\"\n",
        "        \"  - ./Amazon_Data.csv (local)\"\n",
        "    )\n",
        "\n",
        "df = load_dataset()\n",
        "\n",
        "# Apply sampling if needed\n",
        "if SAMPLE_FRAC < 1.0:\n",
        "    df = df.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "    print(f\"✓ Sampled to {len(df):,} rows ({SAMPLE_FRAC*100:.1f}%)\")\n",
        "\n",
        "if MAX_ROWS is not None:\n",
        "    df = df.head(MAX_ROWS)\n",
        "    print(f\"✓ Limited to {len(df):,} rows (MAX_ROWS={MAX_ROWS})\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Could not find Amazon_Data.csv in any expected location.\nPlease place Amazon_Data.csv in one of:\n  - /content/drive/MyDrive/Amazon_Data.csv (Google Drive)\n  - /content/Amazon_Data.csv (Colab upload)\n  - ./Amazon_Data.csv (local)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-76334683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Apply sampling if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-76334683.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;34m\"Could not find Amazon_Data.csv in any expected location.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;34m\"Please place Amazon_Data.csv in one of:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Could not find Amazon_Data.csv in any expected location.\nPlease place Amazon_Data.csv in one of:\n  - /content/drive/MyDrive/Amazon_Data.csv (Google Drive)\n  - /content/Amazon_Data.csv (Colab upload)\n  - ./Amazon_Data.csv (local)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. Cleaning & Basic Preprocessing (text/timestamp/rating, dedupe if needed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clean_data(df):\n",
        "    \"\"\"Clean dataset: remove nulls, convert timestamp, remove empty text.\"\"\"\n",
        "    # Keep only necessary columns\n",
        "    df = df[['text', 'rating', 'timestamp']].copy()\n",
        "    \n",
        "    print(f\"Before cleaning: {len(df):,} rows\")\n",
        "    \n",
        "    # Remove rows with missing values\n",
        "    df = df.dropna()\n",
        "    print(f\"After removing nulls: {len(df):,} rows\")\n",
        "    \n",
        "    # Convert timestamp to datetime\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    df = df.dropna(subset=['timestamp'])\n",
        "    print(f\"After timestamp conversion: {len(df):,} rows\")\n",
        "    \n",
        "    # Remove empty text reviews\n",
        "    df = df[df['text'].astype(str).str.len() > 0].copy()\n",
        "    print(f\"After removing empty text: {len(df):,} rows\")\n",
        "    \n",
        "    # Sort by timestamp (critical for chronological splitting)\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\nDate range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
        "    print(f\"\\nRating distribution:\")\n",
        "    print(df['rating'].value_counts().sort_index())\n",
        "    \n",
        "    return df\n",
        "\n",
        "df = clean_data(df)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-997645696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E. Target Definition (ternary + optional binary helper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_sentiment_labels(df):\n",
        "    \"\"\"\n",
        "    Create sentiment labels from ratings:\n",
        "    - Rating ≤ 2 → Negative (0)\n",
        "    - Rating = 3 → Neutral (1)\n",
        "    - Rating ≥ 4 → Positive (2)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    def rating_to_label(rating):\n",
        "        if rating <= 2:\n",
        "            return 0  # Negative\n",
        "        elif rating == 3:\n",
        "            return 1  # Neutral\n",
        "        else:\n",
        "            return 2  # Positive\n",
        "    \n",
        "    df['sentiment_label'] = df['rating'].apply(rating_to_label)\n",
        "    df['is_negative'] = (df['rating'] <= 2).astype(int)  # Binary helper\n",
        "    \n",
        "    label_counts = df['sentiment_label'].value_counts().sort_index()\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TARGET DISTRIBUTION (Rating-Based Labels)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  Negative (1-2 stars): {label_counts.get(0, 0):,} ({label_counts.get(0, 0)/len(df)*100:.1f}%)\")\n",
        "    print(f\"  Neutral (3 stars):     {label_counts.get(1, 0):,} ({label_counts.get(1, 0)/len(df)*100:.1f}%)\")\n",
        "    print(f\"  Positive (4-5 stars):  {label_counts.get(2, 0):,} ({label_counts.get(2, 0)/len(df)*100:.1f}%)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return df\n",
        "\n",
        "df = create_sentiment_labels(df)\n",
        "y = df['sentiment_label'].values  # Ternary labels (0=negative, 1=neutral, 2=positive)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F. Chronological Split (70/15/15 train/val/test with date ranges printed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def chronological_split(df, y, train_ratio=0.70, val_ratio=0.15):\n",
        "    \"\"\"\n",
        "    Split dataframe chronologically by timestamp.\n",
        "    Returns: df_train, df_val, df_test, y_train, y_val, y_test\n",
        "    \"\"\"\n",
        "    n_total = len(df)\n",
        "    n_train = int(train_ratio * n_total)\n",
        "    n_val = int(val_ratio * n_total)\n",
        "    \n",
        "    # Chronological splits (data already sorted by timestamp)\n",
        "    df_train = df.iloc[:n_train].copy()\n",
        "    df_val = df.iloc[n_train:n_train + n_val].copy()\n",
        "    df_test = df.iloc[n_train + n_val:].copy()\n",
        "    \n",
        "    # Extract targets\n",
        "    y_train = y[:n_train]\n",
        "    y_val = y[n_train:n_train + n_val]\n",
        "    y_test = y[n_train + n_val:]\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"CHRONOLOGICAL SPLIT COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Train set: {len(df_train):,} samples ({len(df_train)/n_total*100:.1f}%)\")\n",
        "    print(f\"  Date range: {df_train['timestamp'].min()} to {df_train['timestamp'].max()}\")\n",
        "    print(f\"\\nValidation set: {len(df_val):,} samples ({len(df_val)/n_total*100:.1f}%)\")\n",
        "    print(f\"  Date range: {df_val['timestamp'].min()} to {df_val['timestamp'].max()}\")\n",
        "    print(f\"\\nTest set: {len(df_test):,} samples ({len(df_test)/n_total*100:.1f}%)\")\n",
        "    print(f\"  Date range: {df_test['timestamp'].min()} to {df_test['timestamp'].max()}\")\n",
        "    \n",
        "    # Assert no temporal overlap\n",
        "    assert df_train['timestamp'].max() <= df_val['timestamp'].min(), \"Train/Val overlap!\"\n",
        "    assert df_val['timestamp'].max() <= df_test['timestamp'].min(), \"Val/Test overlap!\"\n",
        "    print(\"\\n✓ Sanity checks passed: no temporal overlap\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return df_train, df_val, df_test, y_train, y_val, y_test\n",
        "\n",
        "df_train, df_val, df_test, y_train, y_val, y_test = chronological_split(df, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## G. Exploratory Data Analysis (Pre-Training)\n",
        "\n",
        "**Important**: This section uses **training data only** to prevent data leakage. All analyses are descriptive and do not learn thresholds or parameters that would be used for modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Sentiment Class Distribution\n",
        "\n",
        "This bar chart shows the distribution of sentiment labels in the training data. Understanding class balance is crucial for model training, as imbalanced datasets may require class weighting or sampling strategies. The plot displays both counts and percentages for each sentiment class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (Training Data Only)\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPLORATORY DATA ANALYSIS (Pre-Training)\")\n",
        "print(\"Using Training Data Only to Prevent Leakage\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Ensure figures directory exists\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "# 1. Sentiment Class Distribution\n",
        "print(\"\\n1. Creating Sentiment Class Distribution plot...\")\n",
        "sentiment_counts = df_train['sentiment_label'].value_counts().sort_index()\n",
        "sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
        "sentiment_percentages = (sentiment_counts / len(df_train) * 100).round(1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.bar(sentiment_labels, sentiment_counts.values, color=['#d62728', '#ff7f0e', '#2ca02c'], alpha=0.7)\n",
        "ax.set_xlabel('Sentiment Class', fontsize=12)\n",
        "ax.set_ylabel('Count', fontsize=12)\n",
        "ax.set_title('Sentiment Class Distribution (Training Data)', fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add count and percentage labels on bars\n",
        "for i, (bar, count, pct) in enumerate(zip(bars, sentiment_counts.values, sentiment_percentages)):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{count:,}\\n({pct}%)',\n",
        "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'sentiment_class_distribution.png'), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ✓ Saved: {FIGURES_DIR}/sentiment_class_distribution.png\")\n",
        "print(f\"  Distribution: Negative={sentiment_counts.get(0, 0):,} ({sentiment_percentages[0]:.1f}%), \"\n",
        "      f\"Neutral={sentiment_counts.get(1, 0):,} ({sentiment_percentages[1]:.1f}%), \"\n",
        "      f\"Positive={sentiment_counts.get(2, 0):,} ({sentiment_percentages[2]:.1f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Review Length Distribution\n",
        "\n",
        "This analysis examines the distribution of review lengths (word counts) in the training data. The left panel shows the overall distribution, while the right panel shows distributions faceted by sentiment class. This helps identify if review length correlates with sentiment, which could inform feature engineering decisions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2. Review Length Distribution\n",
        "print(\"\\n2. Creating Review Length Distribution plot...\")\n",
        "df_train['review_length'] = df_train['text'].str.split().str.len()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Overall distribution\n",
        "axes[0].hist(df_train['review_length'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "axes[0].set_xlabel('Review Length (Word Count)', fontsize=11)\n",
        "axes[0].set_ylabel('Frequency', fontsize=11)\n",
        "axes[0].set_title('Overall Review Length Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].axvline(df_train['review_length'].median(), color='red', linestyle='--', \n",
        "                label=f'Median: {df_train[\"review_length\"].median():.0f} words')\n",
        "axes[0].legend()\n",
        "\n",
        "# Distribution by sentiment (faceted)\n",
        "for sentiment, label, color in zip([0, 1, 2], ['Negative', 'Neutral', 'Positive'], \n",
        "                                    ['#d62728', '#ff7f0e', '#2ca02c']):\n",
        "    sentiment_data = df_train[df_train['sentiment_label'] == sentiment]['review_length']\n",
        "    axes[1].hist(sentiment_data, bins=30, alpha=0.6, label=label, color=color, edgecolor='black')\n",
        "\n",
        "axes[1].set_xlabel('Review Length (Word Count)', fontsize=11)\n",
        "axes[1].set_ylabel('Frequency', fontsize=11)\n",
        "axes[1].set_title('Review Length Distribution by Sentiment Class', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'review_length_distribution.png'), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ✓ Saved: {FIGURES_DIR}/review_length_distribution.png\")\n",
        "print(f\"  Median review length: {df_train['review_length'].median():.1f} words\")\n",
        "print(f\"  Mean review length: {df_train['review_length'].mean():.1f} words\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Top Tokens per Sentiment (TF-IDF)\n",
        "\n",
        "This analysis identifies the most discriminative tokens for each sentiment class using TF-IDF scores. TF-IDF is fitted on training data only to prevent leakage. The top 10 tokens by average TF-IDF score are displayed for each sentiment class, providing insights into the vocabulary that distinguishes positive, neutral, and negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3. Top Tokens per Sentiment (TF-IDF)\n",
        "print(\"\\n3. Creating Top Tokens per Sentiment (TF-IDF) plot...\")\n",
        "# Fit TF-IDF on training data only\n",
        "tfidf_eda = TfidfVectorizer(max_features=5000, ngram_range=(1, 1), min_df=2, stop_words='english')\n",
        "X_train_tfidf_eda = tfidf_eda.fit_transform(df_train['text'].values)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = tfidf_eda.get_feature_names_out()\n",
        "\n",
        "# Calculate average TF-IDF scores per sentiment class\n",
        "sentiment_names = ['Negative', 'Neutral', 'Positive']\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "for sentiment_idx, (sentiment_label, sentiment_name) in enumerate(zip([0, 1, 2], sentiment_names)):\n",
        "    # Get indices for this sentiment class\n",
        "    sentiment_mask = (df_train['sentiment_label'] == sentiment_label).values\n",
        "    sentiment_tfidf = X_train_tfidf_eda[sentiment_mask]\n",
        "    \n",
        "    # Calculate mean TF-IDF score for each token\n",
        "    mean_scores = np.array(sentiment_tfidf.mean(axis=0)).flatten()\n",
        "    \n",
        "    # Get top 10 tokens\n",
        "    top_indices = np.argsort(mean_scores)[-10:][::-1]\n",
        "    top_tokens = [feature_names[i] for i in top_indices]\n",
        "    top_scores = mean_scores[top_indices]\n",
        "    \n",
        "    # Create horizontal bar chart\n",
        "    axes[sentiment_idx].barh(range(len(top_tokens)), top_scores, color=['#d62728', '#ff7f0e', '#2ca02c'][sentiment_idx], alpha=0.7)\n",
        "    axes[sentiment_idx].set_yticks(range(len(top_tokens)))\n",
        "    axes[sentiment_idx].set_yticklabels(top_tokens)\n",
        "    axes[sentiment_idx].set_xlabel('Average TF-IDF Score', fontsize=10)\n",
        "    axes[sentiment_idx].set_title(f'Top 10 Tokens: {sentiment_name}', fontsize=11, fontweight='bold')\n",
        "    axes[sentiment_idx].grid(axis='x', alpha=0.3)\n",
        "    axes[sentiment_idx].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'top_tokens_per_sentiment_tfidf.png'), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ✓ Saved: {FIGURES_DIR}/top_tokens_per_sentiment_tfidf.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Review Volume Over Time\n",
        "\n",
        "This line plot shows the daily volume of reviews in the training data over time. Understanding temporal patterns in review volume helps identify potential data collection biases, seasonal trends, or periods of high activity that might affect model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4. Review Volume Over Time\n",
        "print(\"\\n4. Creating Review Volume Over Time plot...\")\n",
        "# Aggregate by date (daily)\n",
        "df_train['date'] = df_train['timestamp'].dt.date\n",
        "daily_counts = df_train.groupby('date').size().reset_index(name='count')\n",
        "daily_counts = daily_counts.sort_values('date')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "ax.plot(daily_counts['date'], daily_counts['count'], linewidth=2, color='steelblue', marker='o', markersize=3)\n",
        "ax.set_xlabel('Date', fontsize=12)\n",
        "ax.set_ylabel('Number of Reviews', fontsize=12)\n",
        "ax.set_title('Review Volume Over Time (Daily Aggregation, Training Data)', fontsize=14, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'review_volume_over_time.png'), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ✓ Saved: {FIGURES_DIR}/review_volume_over_time.png\")\n",
        "print(f\"  Date range: {daily_counts['date'].min()} to {daily_counts['date'].max()}\")\n",
        "print(f\"  Total days: {len(daily_counts)}\")\n",
        "print(f\"  Average reviews per day: {daily_counts['count'].mean():.1f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Sentiment Proportion Over Time\n",
        "\n",
        "This stacked area chart shows how the proportion of each sentiment class changes over time. This visualization helps identify temporal shifts in sentiment patterns, which could indicate changes in product quality, customer satisfaction trends, or external factors affecting reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5. Sentiment Proportion Over Time\n",
        "print(\"\\n5. Creating Sentiment Proportion Over Time plot...\")\n",
        "# Aggregate by date and sentiment\n",
        "daily_sentiment = df_train.groupby(['date', 'sentiment_label']).size().reset_index(name='count')\n",
        "daily_sentiment_pivot = daily_sentiment.pivot(index='date', columns='sentiment_label', values='count').fillna(0)\n",
        "daily_sentiment_pivot = daily_sentiment_pivot.sort_index()\n",
        "\n",
        "# Calculate proportions\n",
        "daily_sentiment_pivot['total'] = daily_sentiment_pivot.sum(axis=1)\n",
        "for col in [0, 1, 2]:\n",
        "    daily_sentiment_pivot[col] = daily_sentiment_pivot[col] / daily_sentiment_pivot['total'] * 100\n",
        "\n",
        "# Create stacked area chart\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "colors = ['#d62728', '#ff7f0e', '#2ca02c']\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "ax.stackplot(daily_sentiment_pivot.index, \n",
        "             daily_sentiment_pivot[0], \n",
        "             daily_sentiment_pivot[1], \n",
        "             daily_sentiment_pivot[2],\n",
        "             labels=labels, colors=colors, alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Date', fontsize=12)\n",
        "ax.set_ylabel('Proportion (%)', fontsize=12)\n",
        "ax.set_title('Sentiment Proportion Over Time (Stacked Area Chart, Training Data)', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='upper left')\n",
        "ax.grid(alpha=0.3)\n",
        "ax.set_ylim(0, 100)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'sentiment_proportion_over_time.png'), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ✓ Saved: {FIGURES_DIR}/sentiment_proportion_over_time.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ Exploratory Data Analysis Complete\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## H. Feature Engineering (time-of-day features + TF-IDF pipeline; fit on train only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_time_features(df):\n",
        "    \"\"\"Create time-based features (circular encoding for hour).\"\"\"\n",
        "    df = df.copy()\n",
        "    df['review_hour'] = df['timestamp'].dt.hour\n",
        "    df['review_day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "    \n",
        "    # Circular encoding for hour (preserves 23-0 proximity)\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['review_hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['review_hour'] / 24)\n",
        "    df['is_weekend'] = (df['review_day_of_week'] >= 5).astype(int)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Apply to all splits (no fitting needed)\n",
        "df_train = create_time_features(df_train)\n",
        "df_val = create_time_features(df_val)\n",
        "df_test = create_time_features(df_test)\n",
        "\n",
        "# Time features for modeling\n",
        "TIME_FEATURES = ['hour_sin', 'hour_cos', 'is_weekend']\n",
        "\n",
        "print(\"✓ Time features created on all splits\")\n",
        "print(f\"  Features: {TIME_FEATURES}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## H1. Additional EDA (Hour-based Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Descriptive EDA on TRAIN set only (no leakage)\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPLORATORY DATA ANALYSIS (Train Set Only)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Distribution by hour\n",
        "sentiment_by_hour = df_train.groupby('review_hour').agg({\n",
        "    'sentiment_label': lambda x: (x == 0).mean(),  # Negative rate\n",
        "}).reset_index()\n",
        "sentiment_by_hour.columns = ['hour', 'negative_rate']\n",
        "sentiment_by_hour['n_reviews'] = df_train.groupby('review_hour').size().values\n",
        "sentiment_by_hour['positive_rate'] = df_train.groupby('review_hour')['sentiment_label'].apply(lambda x: (x == 2).mean()).values\n",
        "\n",
        "print(\"\\nSentiment by Hour (Train Set Only):\")\n",
        "print(sentiment_by_hour[['hour', 'n_reviews', 'negative_rate', 'positive_rate']].head(10))\n",
        "\n",
        "# Optional: Plot (skip if in headless mode)\n",
        "try:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(sentiment_by_hour['hour'], sentiment_by_hour['negative_rate'], marker='o', linewidth=2)\n",
        "    plt.xlabel('Hour of Day (0-23)')\n",
        "    plt.ylabel('Negative Review Rate')\n",
        "    plt.title('Negative Review Rate by Hour (Train Set Only)')\n",
        "    plt.xticks(range(0, 24))\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIGURES_DIR, 'negative_rate_by_hour.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"\\n✓ Plot saved to outputs/figures/negative_rate_by_hour.png\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n⚠️  Skipping plot: {e}\")\n",
        "\n",
        "print(\"\\n✓ EDA completed (descriptive only, no thresholds learned)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## I. Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_tfidf_baseline(df_train, df_val, df_test, y_train, y_val, y_test):\n",
        "    \"\"\"\n",
        "    Train TF-IDF + Logistic Regression baseline.\n",
        "    Returns: model dict with tfidf, scaler, clf, time_features\n",
        "    \"\"\"\n",
        "    print(\"Training TF-IDF + Logistic Regression baseline...\")\n",
        "    \n",
        "    # Fit TF-IDF on train only\n",
        "    tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=2)\n",
        "    X_train_tfidf = tfidf.fit_transform(df_train['text'].values)\n",
        "    X_val_tfidf = tfidf.transform(df_val['text'].values)\n",
        "    X_test_tfidf = tfidf.transform(df_test['text'].values)\n",
        "    \n",
        "    # Scale time features (fit on train only)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_time = scaler.fit_transform(df_train[TIME_FEATURES].values)\n",
        "    X_val_time = scaler.transform(df_val[TIME_FEATURES].values)\n",
        "    X_test_time = scaler.transform(df_test[TIME_FEATURES].values)\n",
        "    \n",
        "    # Combine features\n",
        "    X_train_combined = hstack([X_train_tfidf, X_train_time])\n",
        "    X_val_combined = hstack([X_val_tfidf, X_val_time])\n",
        "    X_test_combined = hstack([X_test_tfidf, X_test_time])\n",
        "    \n",
        "    # Train classifier\n",
        "    clf = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, class_weight='balanced')\n",
        "    clf.fit(X_train_combined, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_val_pred = clf.predict(X_val_combined)\n",
        "    y_test_pred = clf.predict(X_test_combined)\n",
        "    \n",
        "    # Metrics\n",
        "    val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
        "    val_acc = accuracy_score(y_val, y_val_pred)\n",
        "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "    \n",
        "    print(f\"  Validation F1 (macro): {val_f1:.4f}\")\n",
        "    print(f\"  Validation Accuracy: {val_acc:.4f}\")\n",
        "    print(f\"  Test F1 (macro): {test_f1:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'tfidf': tfidf,\n",
        "        'scaler': scaler,\n",
        "        'clf': clf,\n",
        "        'time_features': TIME_FEATURES,\n",
        "        'val_f1': val_f1,\n",
        "        'val_acc': val_acc,\n",
        "        'test_f1': test_f1,\n",
        "        'test_acc': test_acc,\n",
        "        'y_val_pred': y_val_pred,\n",
        "        'y_test_pred': y_test_pred\n",
        "    }\n",
        "\n",
        "baseline_model = run_tfidf_baseline(df_train, df_val, df_test, y_train, y_val, y_test)\n",
        "print(\"\\n✓ Baseline model complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Prediction Confidence Distribution\n",
        "\n",
        "This histogram shows the distribution of prediction confidence (softmax probabilities) for each predicted sentiment class. High confidence in correct predictions and low confidence in incorrect predictions indicate a well-calibrated model. This analysis helps identify if the model is overconfident or underconfident in its predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## J. Transformer Fine-tuning (standard HF classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Error Type Breakdown\n",
        "\n",
        "This bar chart shows the most frequent misclassification patterns (true label → predicted label). Understanding which classes are most commonly confused helps identify systematic errors and can guide model improvements, such as focusing on difficult class boundaries or adjusting class weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_distilbert_classifier(df_train, df_val, df_test, y_train, y_val, y_test):\n",
        "    \"\"\"\n",
        "    Fine-tune DistilBERT for ternary sentiment classification.\n",
        "    \"\"\"\n",
        "    print(\"Fine-tuning DistilBERT for sentiment classification...\")\n",
        "    \n",
        "    model_name = \"distilbert-base-uncased\"\n",
        "    \n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=3\n",
        "    ).to(DEVICE)\n",
        "    \n",
        "    # Tokenize datasets\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['text'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=512\n",
        "        )\n",
        "    \n",
        "    train_dataset = Dataset.from_dict({'text': df_train['text'].tolist(), 'label': y_train.tolist()})\n",
        "    val_dataset = Dataset.from_dict({'text': df_val['text'].tolist(), 'label': y_val.tolist()})\n",
        "    test_dataset = Dataset.from_dict({'text': df_test['text'].tolist(), 'label': y_test.tolist()})\n",
        "    \n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "    \n",
        "    # Compute metrics\n",
        "    metric = evaluate.load(\"f1\")\n",
        "    \n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        return {\n",
        "            'f1': metric.compute(predictions=predictions, references=labels, average='macro')['f1'],\n",
        "            'accuracy': accuracy_score(labels, predictions)\n",
        "        }\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(MODELS_DIR, \"distilbert_sentiment\"),\n",
        "        num_train_epochs=EPOCHS_BERT,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        logging_steps=100,\n",
        "        seed=RANDOM_STATE,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    trainer.train()\n",
        "    \n",
        "    # Evaluate\n",
        "    val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
        "    test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "    \n",
        "    # Get predictions\n",
        "    val_predictions = trainer.predict(val_dataset)\n",
        "    test_predictions = trainer.predict(test_dataset)\n",
        "    y_val_pred = np.argmax(val_predictions.predictions, axis=1)\n",
        "    y_test_pred = np.argmax(test_predictions.predictions, axis=1)\n",
        "    \n",
        "    print(f\"  Validation F1 (macro): {val_results['eval_f1']:.4f}\")\n",
        "    print(f\"  Validation Accuracy: {val_results['eval_accuracy']:.4f}\")\n",
        "    print(f\"  Test F1 (macro): {test_results['eval_f1']:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
        "    \n",
        "    # Save model\n",
        "    model.save_pretrained(os.path.join(MODELS_DIR, \"distilbert_sentiment\"))\n",
        "    tokenizer.save_pretrained(os.path.join(MODELS_DIR, \"distilbert_sentiment\"))\n",
        "    \n",
        "    return {\n",
        "        'model': model,\n",
        "        'tokenizer': tokenizer,\n",
        "        'val_f1': val_results['eval_f1'],\n",
        "        'val_acc': val_results['eval_accuracy'],\n",
        "        'test_f1': test_results['eval_f1'],\n",
        "        'test_acc': test_results['eval_accuracy'],\n",
        "        'y_val_pred': y_val_pred,\n",
        "        'y_test_pred': y_test_pred\n",
        "    }\n",
        "\n",
        "# Uncomment to run DistilBERT fine-tuning (may take time)\n",
        "# bert_model = run_distilbert_classifier(df_train, df_val, df_test, y_train, y_val, y_test)\n",
        "# print(\"\\n✓ DistilBERT fine-tuning complete\")\n",
        "print(\"⚠️  DistilBERT fine-tuning skipped (uncomment to run)\")\n",
        "bert_model = None  # Placeholder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Prediction Confidence Distribution\n",
        "\n",
        "This histogram shows the distribution of prediction confidence (softmax probabilities) for each predicted sentiment class. High confidence in correct predictions and low confidence in incorrect predictions indicate a well-calibrated model. This analysis helps identify if the model is overconfident or underconfident in its predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## M. Post-Training Analysis and Error Diagnostics\n",
        "\n",
        "**Important**: This section uses **test data only** for post-training analysis. All diagnostics are performed on held-out test data to understand model behavior and identify error patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Error Type Breakdown\n",
        "\n",
        "This bar chart shows the most frequent misclassification patterns (true label → predicted label). Understanding which classes are most commonly confused helps identify systematic errors and can guide model improvements, such as focusing on difficult class boundaries or adjusting class weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Post-Training Analysis and Error Diagnostics (Test Data Only)\n",
        "print(\"=\" * 60)\n",
        "print(\"POST-TRAINING ANALYSIS AND ERROR DIAGNOSTICS\")\n",
        "print(\"Using Test Data Only\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Ensure figures directory exists\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "# Calculate review lengths for test data\n",
        "df_test['review_length'] = df_test['text'].str.split().str.len()\n",
        "\n",
        "# 1. Prediction Confidence Distribution\n",
        "print(\"\\n1. Creating Prediction Confidence Distribution plot...\")\n",
        "# Get prediction probabilities from baseline model\n",
        "baseline_proba = baseline_model['clf'].predict_proba(\n",
        "    hstack([\n",
        "        baseline_model['tfidf'].transform(df_test['text'].values),\n",
        "        baseline_model['scaler'].transform(df_test[TIME_FEATURES].values)\n",
        "    ])\n",
        ")\n",
        "baseline_pred = baseline_model['y_test_pred']\n",
        "\n",
        "# Create figure with subplots for each predicted class\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
        "colors = ['#d62728', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "for i, (label, color) in enumerate(zip(sentiment_labels, colors)):\n",
        "    # Get predictions for this class\n",
        "    mask = baseline_pred == i\n",
        "    if mask.sum() > 0:\n",
        "        confidences = baseline_proba[mask, i]\n",
        "        axes[i].hist(confidences, bins=30, color=color, alpha=0.7, edgecolor='black')\n",
        "        axes[i].set_xlabel('Prediction Confidence (Probability)', fontsize=11)\n",
        "        axes[i].set_ylabel('Frequency', fontsize=11)\n",
        "        axes[i].set_title(f'Confidence Distribution: Predicted {label}\\n(n={mask.sum()})', \n",
        "                         fontsize=12, fontweight='bold')\n",
        "        axes[i].axvline(confidences.mean(), color='red', linestyle='--', \n",
        "                       label=f'Mean: {confidences.mean():.3f}')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(alpha=0.3)\n",
        "    else:\n",
        "        axes[i].text(0.5, 0.5, f'No predictions for {label}', \n",
        "                    ha='center', va='center', transform=axes[i].transAxes)\n",
        "        axes[i].set_title(f'Confidence Distribution: Predicted {label}', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'prediction_confidence_distribution.png'), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ✓ Saved: {FIGURES_DIR}/prediction_confidence_distribution.png\")\n",
        "print(\"  This plot shows how confident the model is in its predictions for each class.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Review Length vs Classification Error\n",
        "\n",
        "This boxplot compares the distribution of review lengths for correctly and incorrectly classified reviews. If incorrect predictions are associated with very short or very long reviews, this suggests that review length is a factor in classification difficulty. This insight can inform feature engineering or data preprocessing strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Model Disagreement Analysis\n",
        "\n",
        "This bar chart shows cases where different models (baseline, DistilBERT, and LLaMA) make different predictions for the same test sample. High disagreement indicates challenging cases where models are uncertain. These cases may represent ambiguous reviews that require human annotation or additional features. This analysis is only available when multiple models are trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Model Disagreement Analysis\n",
        "\n",
        "This bar chart shows cases where different models (baseline, DistilBERT, and LLaMA) make different predictions for the same test sample. High disagreement indicates challenging cases where models are uncertain. These cases may represent ambiguous reviews that require human annotation or additional features. This analysis is only available when multiple models are trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2. Error Type Breakdown\n",
        "print(\"\\n2. Creating Error Type Breakdown plot...\")\n",
        "# Identify misclassifications\n",
        "errors = []\n",
        "for true_label, pred_label in zip(y_test, baseline_pred):\n",
        "    if true_label != pred_label:\n",
        "        true_name = sentiment_labels[true_label]\n",
        "        pred_name = sentiment_labels[pred_label]\n",
        "        errors.append(f\"{true_name} → {pred_name}\")\n",
        "\n",
        "if len(errors) > 0:\n",
        "    error_counts = pd.Series(errors).value_counts()\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.barh(range(len(error_counts)), error_counts.values, color='coral', alpha=0.7)\n",
        "    ax.set_yticks(range(len(error_counts)))\n",
        "    ax.set_yticklabels(error_counts.index)\n",
        "    ax.set_xlabel('Number of Misclassifications', fontsize=12)\n",
        "    ax.set_ylabel('Error Type (True → Predicted)', fontsize=12)\n",
        "    ax.set_title('Most Frequent Misclassification Pairs (Test Data)', fontsize=14, fontweight='bold')\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    ax.invert_yaxis()\n",
        "    \n",
        "    # Add count labels\n",
        "    for i, (bar, count) in enumerate(zip(bars, error_counts.values)):\n",
        "        ax.text(bar.get_width(), bar.get_y() + bar.get_height()/2,\n",
        "                f' {count:,}',\n",
        "                va='center', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIGURES_DIR, 'error_type_breakdown.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"  ✓ Saved: {FIGURES_DIR}/error_type_breakdown.png\")\n",
        "    print(f\"  Total misclassifications: {len(errors):,} ({len(errors)/len(y_test)*100:.1f}%)\")\n",
        "    print(f\"  Most common error: {error_counts.index[0]} ({error_counts.iloc[0]:,} cases)\")\n",
        "else:\n",
        "    print(\"  ⚠️  No misclassifications found (perfect accuracy)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3. Review Length vs Classification Error\n",
        "print(\"\\n3. Creating Review Length vs Classification Error plot...\")\n",
        "# Identify correct and incorrect predictions\n",
        "df_test['is_correct'] = (y_test == baseline_pred).astype(int)\n",
        "df_test['prediction_status'] = df_test['is_correct'].map({1: 'Correct', 0: 'Incorrect'})\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "df_test.boxplot(column='review_length', by='prediction_status', ax=ax, \n",
        "                patch_artist=True, boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
        "ax.set_xlabel('Prediction Status', fontsize=12)\n",
        "ax.set_ylabel('Review Length (Word Count)', fontsize=12)\n",
        "ax.set_title('Review Length Distribution: Correct vs Incorrect Predictions', fontsize=14, fontweight='bold')\n",
        "ax.set_xticklabels(['Correct', 'Incorrect'])\n",
        "plt.suptitle('')  # Remove default title\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Add statistics text\n",
        "correct_lengths = df_test[df_test['is_correct'] == 1]['review_length']\n",
        "incorrect_lengths = df_test[df_test['is_correct'] == 0]['review_length']\n",
        "stats_text = f\"Correct: μ={correct_lengths.mean():.1f}, median={correct_lengths.median():.1f}\\n\"\n",
        "stats_text += f\"Incorrect: μ={incorrect_lengths.mean():.1f}, median={incorrect_lengths.median():.1f}\"\n",
        "ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
        "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "        fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'review_length_vs_classification_error.png'), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ✓ Saved: {FIGURES_DIR}/review_length_vs_classification_error.png\")\n",
        "print(f\"  Mean length (correct): {correct_lengths.mean():.1f} words\")\n",
        "print(f\"  Mean length (incorrect): {incorrect_lengths.mean():.1f} words\")\n",
        "print(\"  This plot helps identify if review length is correlated with classification errors.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4. Model Disagreement Analysis\n",
        "print(\"\\n4. Creating Model Disagreement Analysis plot...\")\n",
        "# Collect predictions from all available models\n",
        "model_predictions = {\n",
        "    'Baseline (TF-IDF+LR)': baseline_pred\n",
        "}\n",
        "\n",
        "if bert_model is not None:\n",
        "    model_predictions['DistilBERT'] = bert_model['y_test_pred']\n",
        "\n",
        "if unsloth_model is not None:\n",
        "    model_predictions['LLaMA (Unsloth)'] = unsloth_model['y_test_pred_numeric']\n",
        "\n",
        "# Analyze disagreements\n",
        "if len(model_predictions) > 1:\n",
        "    # Create agreement matrix\n",
        "    model_names = list(model_predictions.keys())\n",
        "    disagreements = []\n",
        "    \n",
        "    for i in range(len(y_test)):\n",
        "        predictions = [model_predictions[name][i] for name in model_names]\n",
        "        # Check if all predictions are the same\n",
        "        if len(set(predictions)) > 1:\n",
        "            disagreements.append(i)\n",
        "    \n",
        "    # Count disagreement patterns\n",
        "    disagreement_patterns = {}\n",
        "    for idx in disagreements:\n",
        "        preds = tuple([model_predictions[name][idx] for name in model_names])\n",
        "        preds_str = ' vs '.join([f\"{name}: {sentiment_labels[p]}\" \n",
        "                                 for name, p in zip(model_names, preds)])\n",
        "        disagreement_patterns[preds_str] = disagreement_patterns.get(preds_str, 0) + 1\n",
        "    \n",
        "    if len(disagreement_patterns) > 0:\n",
        "        disagreement_df = pd.Series(disagreement_patterns).sort_values(ascending=False)\n",
        "        \n",
        "        fig, ax = plt.subplots(figsize=(14, max(6, len(disagreement_df) * 0.5)))\n",
        "        bars = ax.barh(range(len(disagreement_df)), disagreement_df.values, color='steelblue', alpha=0.7)\n",
        "        ax.set_yticks(range(len(disagreement_df)))\n",
        "        ax.set_yticklabels(disagreement_df.index, fontsize=9)\n",
        "        ax.set_xlabel('Number of Cases', fontsize=12)\n",
        "        ax.set_ylabel('Disagreement Pattern', fontsize=12)\n",
        "        ax.set_title(f'Model Disagreement Analysis (Test Data)\\nTotal disagreements: {len(disagreements):,} ({len(disagreements)/len(y_test)*100:.1f}%)', \n",
        "                    fontsize=14, fontweight='bold')\n",
        "        ax.grid(axis='x', alpha=0.3)\n",
        "        ax.invert_yaxis()\n",
        "        \n",
        "        # Add count labels\n",
        "        for i, (bar, count) in enumerate(zip(bars, disagreement_df.values)):\n",
        "            ax.text(bar.get_width(), bar.get_y() + bar.get_height()/2,\n",
        "                    f' {count:,}',\n",
        "                    va='center', fontsize=9, fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(FIGURES_DIR, 'model_disagreement_analysis.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"  ✓ Saved: {FIGURES_DIR}/model_disagreement_analysis.png\")\n",
        "        print(f\"  Total disagreement cases: {len(disagreements):,} ({len(disagreements)/len(y_test)*100:.1f}%)\")\n",
        "        print(f\"  Most common disagreement: {disagreement_df.index[0]} ({disagreement_df.iloc[0]:,} cases)\")\n",
        "    else:\n",
        "        print(\"  ✓ All models agree on all test cases\")\n",
        "else:\n",
        "    print(\"  ⚠️  Only one model available. Need at least 2 models for disagreement analysis.\")\n",
        "    print(\"  Uncomment DistilBERT or Unsloth fine-tuning sections to enable this analysis.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ Post-Training Analysis Complete\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K. Unsloth Fine-tuning (from week5_unsloth_sentiment.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install Unsloth (Colab-ready)\n",
        "try:\n",
        "    from unsloth import FastLanguageModel\n",
        "    from trl import SFTTrainer\n",
        "    print(\"✓ Unsloth already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing Unsloth...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth[colab-new]\", \"-q\", \"--no-deps\"])\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"xformers<0.0.27\", \"trl<0.9.0\", \"peft<0.10.0\", \"bitsandbytes<0.43.0\", \"-q\"])\n",
        "    from unsloth import FastLanguageModel\n",
        "    from trl import SFTTrainer\n",
        "    print(\"✓ Unsloth installed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def label_to_string(label):\n",
        "    \"\"\"Convert numeric label to string.\"\"\"\n",
        "    return {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}[label]\n",
        "\n",
        "def string_to_label(label_str):\n",
        "    \"\"\"Convert string label to numeric.\"\"\"\n",
        "    label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "    label_str_lower = label_str.strip().lower()\n",
        "    for key, value in label_map.items():\n",
        "        if key.lower() in label_str_lower or label_str_lower in key.lower():\n",
        "            return value\n",
        "    return 1  # Default to Neutral\n",
        "\n",
        "def create_instruction_prompt(text):\n",
        "    \"\"\"Create instruction prompt for sentiment classification.\"\"\"\n",
        "    return f\"\"\"Classify the sentiment of this review as one of: Negative, Neutral, Positive.\n",
        "\n",
        "Review: {text}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def prepare_unsloth_datasets(df_split, y_split):\n",
        "    \"\"\"Prepare dataset in instruction format for Unsloth.\"\"\"\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for idx, row in df_split.iterrows():\n",
        "        instruction = create_instruction_prompt(row['text'])\n",
        "        texts.append(instruction)\n",
        "        labels.append(label_to_string(y_split[idx]))\n",
        "    \n",
        "    return Dataset.from_dict({'text': texts, 'label': labels})\n",
        "\n",
        "print(\"✓ Unsloth helper functions defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_unsloth_finetune(df_train, df_val, df_test, y_train, y_val, y_test):\n",
        "    \"\"\"Fine-tune Llama-3.1-8B with Unsloth for sentiment classification.\"\"\"\n",
        "    print(\"Fine-tuning Llama-3.1-8B with Unsloth...\")\n",
        "    \n",
        "    # Load model\n",
        "    model_name = \"unsloth/llama-3.1-8b-bnb-4bit\"\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=512,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    \n",
        "    # Add LoRA adapters\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=16,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                       \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=True,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "    \n",
        "    # Prepare datasets\n",
        "    train_dataset = prepare_unsloth_datasets(df_train, y_train)\n",
        "    val_dataset = prepare_unsloth_datasets(df_val, y_val)\n",
        "    test_dataset = prepare_unsloth_datasets(df_test, y_test)\n",
        "    \n",
        "    # Format for training\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    \n",
        "    def format_dataset(examples):\n",
        "        inputs = examples['text']\n",
        "        outputs = examples['label']\n",
        "        texts = [f\"{inp}{out}\" for inp, out in zip(inputs, outputs)]\n",
        "        tokenized = tokenizer(texts, truncation=True, max_length=512, padding=False)\n",
        "        tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "        return tokenized\n",
        "    \n",
        "    train_dataset_formatted = train_dataset.map(format_dataset, batched=True, remove_columns=train_dataset.column_names)\n",
        "    val_dataset_formatted = val_dataset.map(format_dataset, batched=True, remove_columns=val_dataset.column_names)\n",
        "    test_dataset_formatted = test_dataset.map(format_dataset, batched=True, remove_columns=test_dataset.column_names)\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=50,\n",
        "        num_train_epochs=EPOCHS_UNSLOTH,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=100,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        output_dir=os.path.join(MODELS_DIR, \"unsloth_sentiment_model\"),\n",
        "        optim=\"adamw_8bit\",\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\",\n",
        "        seed=RANDOM_STATE,\n",
        "    )\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset_formatted,\n",
        "        eval_dataset=val_dataset_formatted,\n",
        "        args=training_args,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=512,\n",
        "        packing=False,\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    trainer_stats = trainer.train()\n",
        "    print(f\"  Training loss: {trainer_stats.training_loss:.4f}\")\n",
        "    \n",
        "    # Save model\n",
        "    model.save_pretrained(os.path.join(MODELS_DIR, \"unsloth_sentiment_model\"))\n",
        "    tokenizer.save_pretrained(os.path.join(MODELS_DIR, \"unsloth_sentiment_model\"))\n",
        "    \n",
        "    # Inference function\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    \n",
        "    def predict_sentiment(text):\n",
        "        prompt = create_instruction_prompt(text)\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\", truncation=True, max_length=512).to(DEVICE)\n",
        "        outputs = model.generate(**inputs, max_new_tokens=5, temperature=0.0, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if \"Answer:\" in generated_text:\n",
        "            answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "        else:\n",
        "            answer = generated_text.strip()\n",
        "        answer_words = answer.split()\n",
        "        if len(answer_words) > 0:\n",
        "            predicted_label = answer_words[0].strip()\n",
        "        else:\n",
        "            predicted_label = answer.strip()\n",
        "        return label_to_string(string_to_label(predicted_label))\n",
        "    \n",
        "    # Evaluate (sample for speed)\n",
        "    sample_size = min(1000, len(df_test))\n",
        "    test_sample_idx = np.random.choice(len(df_test), sample_size, replace=False)\n",
        "    test_texts_sample = df_test.iloc[test_sample_idx]['text'].values\n",
        "    y_test_true_sample = y_test[test_sample_idx]\n",
        "    \n",
        "    print(f\"  Evaluating on {sample_size} test samples...\")\n",
        "    y_test_pred_sample = []\n",
        "    for text in test_texts_sample:\n",
        "        pred = predict_sentiment(text)\n",
        "        y_test_pred_sample.append(pred)\n",
        "    \n",
        "    y_test_pred_numeric = np.array([string_to_label(pred) for pred in y_test_pred_sample])\n",
        "    y_test_true_numeric = np.array([string_to_label(label_to_string(label)) for label in y_test_true_sample])\n",
        "    \n",
        "    test_f1 = f1_score(y_test_true_numeric, y_test_pred_numeric, average='macro')\n",
        "    test_acc = accuracy_score(y_test_true_numeric, y_test_pred_numeric)\n",
        "    \n",
        "    print(f\"  Test F1 (macro): {test_f1:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'model': model,\n",
        "        'tokenizer': tokenizer,\n",
        "        'predict_sentiment': predict_sentiment,\n",
        "        'test_f1': test_f1,\n",
        "        'test_acc': test_acc,\n",
        "        'y_test_pred_numeric': y_test_pred_numeric,\n",
        "        'y_test_true_numeric': y_test_true_numeric\n",
        "    }\n",
        "\n",
        "# Uncomment to run Unsloth fine-tuning (requires GPU, may take time)\n",
        "# unsloth_model = run_unsloth_finetune(df_train, df_val, df_test, y_train, y_val, y_test)\n",
        "# print(\"\\n✓ Unsloth fine-tuning complete\")\n",
        "print(\"⚠️  Unsloth fine-tuning skipped (uncomment to run)\")\n",
        "unsloth_model = None  # Placeholder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L. Final Comparison & Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_and_save(baseline_model, bert_model, unsloth_model, y_test, output_dir):\n",
        "    \"\"\"Create comparison table and save results.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Baseline\n",
        "    results.append({\n",
        "        'Model': 'TF-IDF + Logistic Regression',\n",
        "        'Test F1 (macro)': baseline_model['test_f1'],\n",
        "        'Test Accuracy': baseline_model['test_acc']\n",
        "    })\n",
        "    \n",
        "    # DistilBERT\n",
        "    if bert_model is not None:\n",
        "        results.append({\n",
        "            'Model': 'DistilBERT (Fine-tuned)',\n",
        "            'Test F1 (macro)': bert_model['test_f1'],\n",
        "            'Test Accuracy': bert_model['test_acc']\n",
        "        })\n",
        "    \n",
        "    # Unsloth\n",
        "    if unsloth_model is not None:\n",
        "        results.append({\n",
        "            'Model': 'Unsloth (Llama-3.1-8B)',\n",
        "            'Test F1 (macro)': unsloth_model['test_f1'],\n",
        "            'Test Accuracy': unsloth_model['test_acc']\n",
        "        })\n",
        "    \n",
        "    # Create DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"FINAL MODEL COMPARISON (Test Set)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(results_df.to_string(index=False))\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Save to CSV\n",
        "    results_df.to_csv(os.path.join(output_dir, 'model_comparison.csv'), index=False)\n",
        "    print(f\"\\n✓ Results saved to {output_dir}/model_comparison.csv\")\n",
        "    \n",
        "    # Confusion matrices\n",
        "    if baseline_model is not None:\n",
        "        cm_baseline = confusion_matrix(y_test, baseline_model['y_test_pred'])\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                    yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.title('Confusion Matrix - TF-IDF Baseline')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, 'figures', 'confusion_matrix_baseline.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    \n",
        "    if bert_model is not None:\n",
        "        cm_bert = confusion_matrix(y_test, bert_model['y_test_pred'])\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                    yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.title('Confusion Matrix - DistilBERT')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, 'figures', 'confusion_matrix_distilbert.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    \n",
        "    if unsloth_model is not None:\n",
        "        cm_unsloth = confusion_matrix(unsloth_model['y_test_true_numeric'], unsloth_model['y_test_pred_numeric'])\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm_unsloth, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                    yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.title('Confusion Matrix - Unsloth')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, 'figures', 'confusion_matrix_unsloth.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    \n",
        "    print(f\"\\n✓ Confusion matrices saved to {output_dir}/figures/\")\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "results_df = evaluate_and_save(baseline_model, bert_model, unsloth_model, y_test, OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "**Pipeline Complete** ✓\n",
        "\n",
        "- Data loaded and cleaned\n",
        "- Chronological split (70/15/15)\n",
        "- Baseline model trained (TF-IDF + Logistic Regression)\n",
        "- Transformer fine-tuning available (DistilBERT)\n",
        "- Unsloth fine-tuning available (Llama-3.1-8B)\n",
        "- Results saved to `outputs/`\n",
        "\n",
        "**Note**: To run DistilBERT or Unsloth fine-tuning, uncomment the respective sections above.\n",
        "\n",
        "**Reproducibility**: All results use seed=319302\n",
        "\n",
        "**Data Leakage Prevention**: ✓ Chronological splits, transforms fit on train only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === FAST EVALUATION + SAVING OUTPUTS ===\n",
        "import shutil\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Save LoRA adapter + tokenizer\n",
        "model.save_pretrained(MODEL_DIR)\n",
        "tokenizer.save_pretrained(MODEL_DIR)\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Build prompts (no gold label)\n",
        "def build_prompt(review_text: str) -> str:\n",
        "    return INSTRUCTION_TEMPLATE.format(review=review_text)\n",
        "\n",
        "# Robust parsing\n",
        "LABEL_REGEX = re.compile(r\"\\b(negative|neutral|positive)\\b\", re.IGNORECASE)\n",
        "\n",
        "def parse_prediction(output_text: str):\n",
        "    try:\n",
        "        match = LABEL_REGEX.search(output_text)\n",
        "        if match:\n",
        "            label_str = match.group(1).lower()\n",
        "            return label_map_inv[label_str]\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Run evaluation\n",
        "pred_rows = []\n",
        "correct = 0\n",
        "\n",
        "for _, row in test_subset.iterrows():\n",
        "    prompt = build_prompt(row[\"text\"])\n",
        "    gold_id = int(row[\"label\"])\n",
        "    gold_label = label_map[gold_id]\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LEN).to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        max_new_tokens=3,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    pred_id = parse_prediction(raw_output)\n",
        "\n",
        "    if pred_id is None:\n",
        "        pred_label = \"Unknown\"\n",
        "        pred_id_for_metrics = 99\n",
        "    else:\n",
        "        pred_label = label_map[pred_id]\n",
        "        pred_id_for_metrics = pred_id\n",
        "\n",
        "    if pred_id == gold_id:\n",
        "        correct += 1\n",
        "\n",
        "    pred_rows.append({\n",
        "        \"text\": row[\"text\"],\n",
        "        \"gold_label\": gold_label,\n",
        "        \"pred_label\": pred_label,\n",
        "        \"raw_output\": raw_output,\n",
        "        \"pred_id\": pred_id_for_metrics,\n",
        "        \"gold_id\": gold_id,\n",
        "    })\n",
        "\n",
        "# Metrics\n",
        "accuracy = correct / len(test_subset) if len(test_subset) > 0 else 0.0\n",
        "f1_macro = f1_score(\n",
        "    [r[\"gold_id\"] for r in pred_rows],\n",
        "    [r[\"pred_id\"] for r in pred_rows],\n",
        "    labels=[0, 1, 2],\n",
        "    average=\"macro\",\n",
        "    zero_division=0,\n",
        ")\n",
        "\n",
        "metrics = {\n",
        "    \"accuracy\": accuracy,\n",
        "    \"macro_f1\": f1_macro,\n",
        "    \"n_eval\": len(test_subset),\n",
        "}\n",
        "\n",
        "print(\"Evaluation metrics:\")\n",
        "print(metrics)\n",
        "\n",
        "# Save outputs\n",
        "pred_df = pd.DataFrame(pred_rows)[[\"text\", \"gold_label\", \"pred_label\", \"raw_output\"]]\n",
        "pred_df.to_csv(os.path.join(OUTPUT_DIR, \"unsloth_pred_examples.csv\"), index=False)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"unsloth_metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# Zip outputs\n",
        "shutil.make_archive(\"outputs_week7_unsloth\", \"zip\", OUTPUT_DIR)\n",
        "print(\"✓ Saved outputs/unsloth_metrics.json\")\n",
        "print(\"✓ Saved outputs/unsloth_pred_examples.csv\")\n",
        "print(\"✓ Created outputs_week7_unsloth.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}