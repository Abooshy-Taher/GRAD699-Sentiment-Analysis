{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Final Sentiment Analysis Pipeline\n",
    "\n",
    "## End-to-End Thesis-Quality Experiment Pipeline\n",
    "\n",
    "This notebook combines all components into a clean, reproducible, Colab-ready pipeline:\n",
    "- Data loading & cleaning\n",
    "- Chronological splitting (prevents temporal leakage)\n",
    "- Feature engineering (TF-IDF + time features, fit on train only)\n",
    "- Baseline models (TF-IDF + Logistic Regression)\n",
    "- Transformer fine-tuning (DistilBERT)\n",
    "- Unsloth fine-tuning (Llama-3.1-8B with LoRA)\n",
    "- Final evaluation & comparison\n",
    "\n",
    "**Reproducibility**: seed=319302 throughout\n",
    "**Data Leakage Prevention**: Chronological splits, transforms fit on train only\n",
    "**Colab-Ready**: Auto-detects data paths, includes GPU checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Environment Setup (Colab installs, GPU check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Colab\n",
    "# Skip if running locally and packages are already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package.split(\"==\")[0].split(\">=\")[0].split(\"[\")[0])\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "\n",
    "# Core ML packages\n",
    "install_if_missing(\"pandas>=2.0.0\")\n",
    "install_if_missing(\"numpy>=1.24.0\")\n",
    "install_if_missing(\"scikit-learn>=1.3.0\")\n",
    "install_if_missing(\"matplotlib>=3.7.0\")\n",
    "install_if_missing(\"seaborn>=0.12.0\")\n",
    "\n",
    "# Transformer packages\n",
    "install_if_missing(\"transformers>=4.40.0\")\n",
    "install_if_missing(\"datasets>=2.18.0\")\n",
    "install_if_missing(\"evaluate>=0.4.1\")\n",
    "install_if_missing(\"accelerate>=0.20.0\")\n",
    "install_if_missing(\"torch>=2.0.0\")\n",
    "\n",
    "print(\"✓ Core packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Mount Google Drive (uncomment if needed)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# print(\"✓ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    print(\"⚠️  CPU mode (GPU recommended for transformer/unsloth training)\")\n",
    "    DEVICE = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Imports & Global Config (seed=319302, paths, flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, accuracy_score\n",
    ")\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Transformer imports\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "RANDOM_STATE = 319302\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "\n",
    "# FAST RUN toggles\n",
    "SAMPLE_FRAC = 1.0  # Use 1.0 for full dataset, < 1.0 for quick tests\n",
    "MAX_ROWS = None  # None for all rows, or set limit for debug\n",
    "EPOCHS_BERT = 1  # Epochs for DistilBERT fine-tuning\n",
    "EPOCHS_UNSLOTH = 1  # Epochs for Unsloth fine-tuning\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "MODELS_DIR = \"models\"\n",
    "FIGURES_DIR = os.path.join(OUTPUT_DIR, \"figures\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports complete\")\n",
    "print(f\"✓ Random seed: {RANDOM_STATE}\")\n",
    "print(f\"✓ Sample fraction: {SAMPLE_FRAC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Load Data (auto-detect paths, show shape/columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load Amazon_Data.csv from multiple possible locations.\n",
    "    Auto-detects Colab paths and local paths.\n",
    "    \"\"\"\n",
    "    possible_paths = [\n",
    "        # Colab paths\n",
    "        \"/content/drive/MyDrive/Amazon_Data.csv\",\n",
    "        \"/content/Amazon_Data.csv\",\n",
    "        # Local paths\n",
    "        \"../Amazon_Data.csv\",\n",
    "        \"Amazon_Data.csv\",\n",
    "        os.path.join(os.path.expanduser(\"~\"), \"Desktop/HU Classes/GRAD699/Sentiment Analysis/Amazon_Data.csv\")\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"✓ Found file at: {path}\")\n",
    "            print(f\"Dataset loaded: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "            return df\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find Amazon_Data.csv in any expected location.\\n\"\n",
    "        \"Please place Amazon_Data.csv in one of:\\n\"\n",
    "        \"  - /content/drive/MyDrive/Amazon_Data.csv (Google Drive)\\n\"\n",
    "        \"  - /content/Amazon_Data.csv (Colab upload)\\n\"\n",
    "        \"  - ./Amazon_Data.csv (local)\"\n",
    "    )\n",
    "\n",
    "df = load_dataset()\n",
    "\n",
    "# Apply sampling if needed\n",
    "if SAMPLE_FRAC < 1.0:\n",
    "    df = df.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    print(f\"✓ Sampled to {len(df):,} rows ({SAMPLE_FRAC*100:.1f}%)\")\n",
    "\n",
    "if MAX_ROWS is not None:\n",
    "    df = df.head(MAX_ROWS)\n",
    "    print(f\"✓ Limited to {len(df):,} rows (MAX_ROWS={MAX_ROWS})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Cleaning & Basic Preprocessing (text/timestamp/rating, dedupe if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean dataset: remove nulls, convert timestamp, remove empty text.\"\"\"\n",
    "    # Keep only necessary columns\n",
    "    df = df[['text', 'rating', 'timestamp']].copy()\n",
    "    \n",
    "    print(f\"Before cleaning: {len(df):,} rows\")\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    df = df.dropna()\n",
    "    print(f\"After removing nulls: {len(df):,} rows\")\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    df = df.dropna(subset=['timestamp'])\n",
    "    print(f\"After timestamp conversion: {len(df):,} rows\")\n",
    "    \n",
    "    # Remove empty text reviews\n",
    "    df = df[df['text'].astype(str).str.len() > 0].copy()\n",
    "    print(f\"After removing empty text: {len(df):,} rows\")\n",
    "    \n",
    "    # Sort by timestamp (critical for chronological splitting)\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nDate range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"\\nRating distribution:\")\n",
    "    print(df['rating'].value_counts().sort_index())\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Target Definition (ternary + optional binary helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_labels(df):\n",
    "    \"\"\"\n",
    "    Create sentiment labels from ratings:\n",
    "    - Rating ≤ 2 → Negative (0)\n",
    "    - Rating = 3 → Neutral (1)\n",
    "    - Rating ≥ 4 → Positive (2)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    def rating_to_label(rating):\n",
    "        if rating <= 2:\n",
    "            return 0  # Negative\n",
    "        elif rating == 3:\n",
    "            return 1  # Neutral\n",
    "        else:\n",
    "            return 2  # Positive\n",
    "    \n",
    "    df['sentiment_label'] = df['rating'].apply(rating_to_label)\n",
    "    df['is_negative'] = (df['rating'] <= 2).astype(int)  # Binary helper\n",
    "    \n",
    "    label_counts = df['sentiment_label'].value_counts().sort_index()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TARGET DISTRIBUTION (Rating-Based Labels)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Negative (1-2 stars): {label_counts.get(0, 0):,} ({label_counts.get(0, 0)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Neutral (3 stars):     {label_counts.get(1, 0):,} ({label_counts.get(1, 0)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Positive (4-5 stars):  {label_counts.get(2, 0):,} ({label_counts.get(2, 0)/len(df)*100:.1f}%)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_sentiment_labels(df)\n",
    "y = df['sentiment_label'].values  # Ternary labels (0=negative, 1=neutral, 2=positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Chronological Split (70/15/15 train/val/test with date ranges printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronological_split(df, y, train_ratio=0.70, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Split dataframe chronologically by timestamp.\n",
    "    Returns: df_train, df_val, df_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    n_total = len(df)\n",
    "    n_train = int(train_ratio * n_total)\n",
    "    n_val = int(val_ratio * n_total)\n",
    "    \n",
    "    # Chronological splits (data already sorted by timestamp)\n",
    "    df_train = df.iloc[:n_train].copy()\n",
    "    df_val = df.iloc[n_train:n_train + n_val].copy()\n",
    "    df_test = df.iloc[n_train + n_val:].copy()\n",
    "    \n",
    "    # Extract targets\n",
    "    y_train = y[:n_train]\n",
    "    y_val = y[n_train:n_train + n_val]\n",
    "    y_test = y[n_train + n_val:]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CHRONOLOGICAL SPLIT COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Train set: {len(df_train):,} samples ({len(df_train)/n_total*100:.1f}%)\")\n",
    "    print(f\"  Date range: {df_train['timestamp'].min()} to {df_train['timestamp'].max()}\")\n",
    "    print(f\"\\nValidation set: {len(df_val):,} samples ({len(df_val)/n_total*100:.1f}%)\")\n",
    "    print(f\"  Date range: {df_val['timestamp'].min()} to {df_val['timestamp'].max()}\")\n",
    "    print(f\"\\nTest set: {len(df_test):,} samples ({len(df_test)/n_total*100:.1f}%)\")\n",
    "    print(f\"  Date range: {df_test['timestamp'].min()} to {df_test['timestamp'].max()}\")\n",
    "    \n",
    "    # Assert no temporal overlap\n",
    "    assert df_train['timestamp'].max() <= df_val['timestamp'].min(), \"Train/Val overlap!\"\n",
    "    assert df_val['timestamp'].max() <= df_test['timestamp'].min(), \"Val/Test overlap!\"\n",
    "    print(\"\\n✓ Sanity checks passed: no temporal overlap\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return df_train, df_val, df_test, y_train, y_val, y_test\n",
    "\n",
    "df_train, df_val, df_test, y_train, y_val, y_test = chronological_split(df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Feature Engineering (time-of-day features + TF-IDF pipeline; fit on train only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df):\n",
    "    \"\"\"Create time-based features (circular encoding for hour).\"\"\"\n",
    "    df = df.copy()\n",
    "    df['review_hour'] = df['timestamp'].dt.hour\n",
    "    df['review_day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    \n",
    "    # Circular encoding for hour (preserves 23-0 proximity)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['review_hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['review_hour'] / 24)\n",
    "    df['is_weekend'] = (df['review_day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to all splits (no fitting needed)\n",
    "df_train = create_time_features(df_train)\n",
    "df_val = create_time_features(df_val)\n",
    "df_test = create_time_features(df_test)\n",
    "\n",
    "# Time features for modeling\n",
    "TIME_FEATURES = ['hour_sin', 'hour_cos', 'is_weekend']\n",
    "\n",
    "print(\"✓ Time features created on all splits\")\n",
    "print(f\"  Features: {TIME_FEATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. EDA (strictly descriptive: label distribution by hour/day/month; no leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive EDA on TRAIN set only (no leakage)\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPLORATORY DATA ANALYSIS (Train Set Only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Distribution by hour\n",
    "sentiment_by_hour = df_train.groupby('review_hour').agg({\n",
    "    'sentiment_label': lambda x: (x == 0).mean(),  # Negative rate\n",
    "}).reset_index()\n",
    "sentiment_by_hour.columns = ['hour', 'negative_rate']\n",
    "sentiment_by_hour['n_reviews'] = df_train.groupby('review_hour').size().values\n",
    "sentiment_by_hour['positive_rate'] = df_train.groupby('review_hour')['sentiment_label'].apply(lambda x: (x == 2).mean()).values\n",
    "\n",
    "print(\"\\nSentiment by Hour (Train Set Only):\")\n",
    "print(sentiment_by_hour[['hour', 'n_reviews', 'negative_rate', 'positive_rate']].head(10))\n",
    "\n",
    "# Optional: Plot (skip if in headless mode)\n",
    "try:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(sentiment_by_hour['hour'], sentiment_by_hour['negative_rate'], marker='o', linewidth=2)\n",
    "    plt.xlabel('Hour of Day (0-23)')\n",
    "    plt.ylabel('Negative Review Rate')\n",
    "    plt.title('Negative Review Rate by Hour (Train Set Only)')\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'negative_rate_by_hour.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"\\n✓ Plot saved to outputs/figures/negative_rate_by_hour.png\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️  Skipping plot: {e}\")\n",
    "\n",
    "print(\"\\n✓ EDA completed (descriptive only, no thresholds learned)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tfidf_baseline(df_train, df_val, df_test, y_train, y_val, y_test):\n",
    "    \"\"\"\n",
    "    Train TF-IDF + Logistic Regression baseline.\n",
    "    Returns: model dict with tfidf, scaler, clf, time_features\n",
    "    \"\"\"\n",
    "    print(\"Training TF-IDF + Logistic Regression baseline...\")\n",
    "    \n",
    "    # Fit TF-IDF on train only\n",
    "    tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=2)\n",
    "    X_train_tfidf = tfidf.fit_transform(df_train['text'].values)\n",
    "    X_val_tfidf = tfidf.transform(df_val['text'].values)\n",
    "    X_test_tfidf = tfidf.transform(df_test['text'].values)\n",
    "    \n",
    "    # Scale time features (fit on train only)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_time = scaler.fit_transform(df_train[TIME_FEATURES].values)\n",
    "    X_val_time = scaler.transform(df_val[TIME_FEATURES].values)\n",
    "    X_test_time = scaler.transform(df_test[TIME_FEATURES].values)\n",
    "    \n",
    "    # Combine features\n",
    "    X_train_combined = hstack([X_train_tfidf, X_train_time])\n",
    "    X_val_combined = hstack([X_val_tfidf, X_val_time])\n",
    "    X_test_combined = hstack([X_test_tfidf, X_test_time])\n",
    "    \n",
    "    # Train classifier\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, class_weight='balanced')\n",
    "    clf.fit(X_train_combined, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_val_pred = clf.predict(X_val_combined)\n",
    "    y_test_pred = clf.predict(X_test_combined)\n",
    "    \n",
    "    # Metrics\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"  Validation F1 (macro): {val_f1:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"  Test F1 (macro): {test_f1:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'tfidf': tfidf,\n",
    "        'scaler': scaler,\n",
    "        'clf': clf,\n",
    "        'time_features': TIME_FEATURES,\n",
    "        'val_f1': val_f1,\n",
    "        'val_acc': val_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_acc': test_acc,\n",
    "        'y_val_pred': y_val_pred,\n",
    "        'y_test_pred': y_test_pred\n",
    "    }\n",
    "\n",
    "baseline_model = run_tfidf_baseline(df_train, df_val, df_test, y_train, y_val, y_test)\n",
    "print(\"\\n✓ Baseline model complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## J. Transformer Fine-tuning (standard HF classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_distilbert_classifier(df_train, df_val, df_test, y_train, y_val, y_test):\n",
    "    \"\"\"\n",
    "    Fine-tune DistilBERT for ternary sentiment classification.\n",
    "    \"\"\"\n",
    "    print(\"Fine-tuning DistilBERT for sentiment classification...\")\n",
    "    \n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=3\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512\n",
    "        )\n",
    "    \n",
    "    train_dataset = Dataset.from_dict({'text': df_train['text'].tolist(), 'label': y_train.tolist()})\n",
    "    val_dataset = Dataset.from_dict({'text': df_val['text'].tolist(), 'label': y_val.tolist()})\n",
    "    test_dataset = Dataset.from_dict({'text': df_test['text'].tolist(), 'label': y_test.tolist()})\n",
    "    \n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metric = evaluate.load(\"f1\")\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return {\n",
    "            'f1': metric.compute(predictions=predictions, references=labels, average='macro')['f1'],\n",
    "            'accuracy': accuracy_score(labels, predictions)\n",
    "        }\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(MODELS_DIR, \"distilbert_sentiment\"),\n",
    "        num_train_epochs=EPOCHS_BERT,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=100,\n",
    "        seed=RANDOM_STATE,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "    test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    \n",
    "    # Get predictions\n",
    "    val_predictions = trainer.predict(val_dataset)\n",
    "    test_predictions = trainer.predict(test_dataset)\n",
    "    y_val_pred = np.argmax(val_predictions.predictions, axis=1)\n",
    "    y_test_pred = np.argmax(test_predictions.predictions, axis=1)\n",
    "    \n",
    "    print(f\"  Validation F1 (macro): {val_results['eval_f1']:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"  Test F1 (macro): {test_results['eval_f1']:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save_pretrained(os.path.join(MODELS_DIR, \"distilbert_sentiment\"))\n",
    "    tokenizer.save_pretrained(os.path.join(MODELS_DIR, \"distilbert_sentiment\"))\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'val_f1': val_results['eval_f1'],\n",
    "        'val_acc': val_results['eval_accuracy'],\n",
    "        'test_f1': test_results['eval_f1'],\n",
    "        'test_acc': test_results['eval_accuracy'],\n",
    "        'y_val_pred': y_val_pred,\n",
    "        'y_test_pred': y_test_pred\n",
    "    }\n",
    "\n",
    "# Uncomment to run DistilBERT fine-tuning (may take time)\n",
    "# bert_model = run_distilbert_classifier(df_train, df_val, df_test, y_train, y_val, y_test)\n",
    "# print(\"\\n✓ DistilBERT fine-tuning complete\")\n",
    "print(\"⚠️  DistilBERT fine-tuning skipped (uncomment to run)\")\n",
    "bert_model = None  # Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K. Unsloth Fine-tuning (from week5_unsloth_sentiment.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth (Colab-ready)\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    from trl import SFTTrainer\n",
    "    print(\"✓ Unsloth already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing Unsloth...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth[colab-new]\", \"-q\", \"--no-deps\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"xformers<0.0.27\", \"trl<0.9.0\", \"peft<0.10.0\", \"bitsandbytes<0.43.0\", \"-q\"])\n",
    "    from unsloth import FastLanguageModel\n",
    "    from trl import SFTTrainer\n",
    "    print(\"✓ Unsloth installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_string(label):\n",
    "    \"\"\"Convert numeric label to string.\"\"\"\n",
    "    return {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}[label]\n",
    "\n",
    "def string_to_label(label_str):\n",
    "    \"\"\"Convert string label to numeric.\"\"\"\n",
    "    label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "    label_str_lower = label_str.strip().lower()\n",
    "    for key, value in label_map.items():\n",
    "        if key.lower() in label_str_lower or label_str_lower in key.lower():\n",
    "            return value\n",
    "    return 1  # Default to Neutral\n",
    "\n",
    "def create_instruction_prompt(text):\n",
    "    \"\"\"Create instruction prompt for sentiment classification.\"\"\"\n",
    "    return f\"\"\"Classify the sentiment of this review as one of: Negative, Neutral, Positive.\n",
    "\n",
    "Review: {text}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "def prepare_unsloth_datasets(df_split, y_split):\n",
    "    \"\"\"Prepare dataset in instruction format for Unsloth.\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for idx, row in df_split.iterrows():\n",
    "        instruction = create_instruction_prompt(row['text'])\n",
    "        texts.append(instruction)\n",
    "        labels.append(label_to_string(y_split[idx]))\n",
    "    \n",
    "    return Dataset.from_dict({'text': texts, 'label': labels})\n",
    "\n",
    "print(\"✓ Unsloth helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_unsloth_finetune(df_train, df_val, df_test, y_train, y_val, y_test):\n",
    "    \"\"\"Fine-tune Llama-3.1-8B with Unsloth for sentiment classification.\"\"\"\n",
    "    print(\"Fine-tuning Llama-3.1-8B with Unsloth...\")\n",
    "    \n",
    "    # Load model\n",
    "    model_name = \"unsloth/llama-3.1-8b-bnb-4bit\"\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=512,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    # Add LoRA adapters\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                       \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=True,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = prepare_unsloth_datasets(df_train, y_train)\n",
    "    val_dataset = prepare_unsloth_datasets(df_val, y_val)\n",
    "    test_dataset = prepare_unsloth_datasets(df_test, y_test)\n",
    "    \n",
    "    # Format for training\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    def format_dataset(examples):\n",
    "        inputs = examples['text']\n",
    "        outputs = examples['label']\n",
    "        texts = [f\"{inp}{out}\" for inp, out in zip(inputs, outputs)]\n",
    "        tokenized = tokenizer(texts, truncation=True, max_length=512, padding=False)\n",
    "        tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "        return tokenized\n",
    "    \n",
    "    train_dataset_formatted = train_dataset.map(format_dataset, batched=True, remove_columns=train_dataset.column_names)\n",
    "    val_dataset_formatted = val_dataset.map(format_dataset, batched=True, remove_columns=val_dataset.column_names)\n",
    "    test_dataset_formatted = test_dataset.map(format_dataset, batched=True, remove_columns=test_dataset.column_names)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=50,\n",
    "        num_train_epochs=EPOCHS_UNSLOTH,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        output_dir=os.path.join(MODELS_DIR, \"unsloth_sentiment_model\"),\n",
    "        optim=\"adamw_8bit\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\",\n",
    "        seed=RANDOM_STATE,\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset_formatted,\n",
    "        eval_dataset=val_dataset_formatted,\n",
    "        args=training_args,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer_stats = trainer.train()\n",
    "    print(f\"  Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save_pretrained(os.path.join(MODELS_DIR, \"unsloth_sentiment_model\"))\n",
    "    tokenizer.save_pretrained(os.path.join(MODELS_DIR, \"unsloth_sentiment_model\"))\n",
    "    \n",
    "    # Inference function\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    def predict_sentiment(text):\n",
    "        prompt = create_instruction_prompt(text)\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\", truncation=True, max_length=512).to(DEVICE)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=5, temperature=0.0, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if \"Answer:\" in generated_text:\n",
    "            answer = generated_text.split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            answer = generated_text.strip()\n",
    "        answer_words = answer.split()\n",
    "        if len(answer_words) > 0:\n",
    "            predicted_label = answer_words[0].strip()\n",
    "        else:\n",
    "            predicted_label = answer.strip()\n",
    "        return label_to_string(string_to_label(predicted_label))\n",
    "    \n",
    "    # Evaluate (sample for speed)\n",
    "    sample_size = min(1000, len(df_test))\n",
    "    test_sample_idx = np.random.choice(len(df_test), sample_size, replace=False)\n",
    "    test_texts_sample = df_test.iloc[test_sample_idx]['text'].values\n",
    "    y_test_true_sample = y_test[test_sample_idx]\n",
    "    \n",
    "    print(f\"  Evaluating on {sample_size} test samples...\")\n",
    "    y_test_pred_sample = []\n",
    "    for text in test_texts_sample:\n",
    "        pred = predict_sentiment(text)\n",
    "        y_test_pred_sample.append(pred)\n",
    "    \n",
    "    y_test_pred_numeric = np.array([string_to_label(pred) for pred in y_test_pred_sample])\n",
    "    y_test_true_numeric = np.array([string_to_label(label_to_string(label)) for label in y_test_true_sample])\n",
    "    \n",
    "    test_f1 = f1_score(y_test_true_numeric, y_test_pred_numeric, average='macro')\n",
    "    test_acc = accuracy_score(y_test_true_numeric, y_test_pred_numeric)\n",
    "    \n",
    "    print(f\"  Test F1 (macro): {test_f1:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'predict_sentiment': predict_sentiment,\n",
    "        'test_f1': test_f1,\n",
    "        'test_acc': test_acc,\n",
    "        'y_test_pred_numeric': y_test_pred_numeric,\n",
    "        'y_test_true_numeric': y_test_true_numeric\n",
    "    }\n",
    "\n",
    "# Uncomment to run Unsloth fine-tuning (requires GPU, may take time)\n",
    "# unsloth_model = run_unsloth_finetune(df_train, df_val, df_test, y_train, y_val, y_test)\n",
    "# print(\"\\n✓ Unsloth fine-tuning complete\")\n",
    "print(\"⚠️  Unsloth fine-tuning skipped (uncomment to run)\")\n",
    "unsloth_model = None  # Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L. Final Comparison & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save(baseline_model, bert_model, unsloth_model, y_test, output_dir):\n",
    "    \"\"\"Create comparison table and save results.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Baseline\n",
    "    results.append({\n",
    "        'Model': 'TF-IDF + Logistic Regression',\n",
    "        'Test F1 (macro)': baseline_model['test_f1'],\n",
    "        'Test Accuracy': baseline_model['test_acc']\n",
    "    })\n",
    "    \n",
    "    # DistilBERT\n",
    "    if bert_model is not None:\n",
    "        results.append({\n",
    "            'Model': 'DistilBERT (Fine-tuned)',\n",
    "            'Test F1 (macro)': bert_model['test_f1'],\n",
    "            'Test Accuracy': bert_model['test_acc']\n",
    "        })\n",
    "    \n",
    "    # Unsloth\n",
    "    if unsloth_model is not None:\n",
    "        results.append({\n",
    "            'Model': 'Unsloth (Llama-3.1-8B)',\n",
    "            'Test F1 (macro)': unsloth_model['test_f1'],\n",
    "            'Test Accuracy': unsloth_model['test_acc']\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINAL MODEL COMPARISON (Test Set)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results_df.to_string(index=False))\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(os.path.join(output_dir, 'model_comparison.csv'), index=False)\n",
    "    print(f\"\\n✓ Results saved to {output_dir}/model_comparison.csv\")\n",
    "    \n",
    "    # Confusion matrices\n",
    "    if baseline_model is not None:\n",
    "        cm_baseline = confusion_matrix(y_test, baseline_model['y_test_pred'])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "                    yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.title('Confusion Matrix - TF-IDF Baseline')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'figures', 'confusion_matrix_baseline.png'), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    if bert_model is not None:\n",
    "        cm_bert = confusion_matrix(y_test, bert_model['y_test_pred'])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "                    yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.title('Confusion Matrix - DistilBERT')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'figures', 'confusion_matrix_distilbert.png'), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    if unsloth_model is not None:\n",
    "        cm_unsloth = confusion_matrix(unsloth_model['y_test_true_numeric'], unsloth_model['y_test_pred_numeric'])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm_unsloth, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "                    yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.title('Confusion Matrix - Unsloth')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'figures', 'confusion_matrix_unsloth.png'), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"\\n✓ Confusion matrices saved to {output_dir}/figures/\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "results_df = evaluate_and_save(baseline_model, bert_model, unsloth_model, y_test, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**Pipeline Complete** ✓\n",
    "\n",
    "- Data loaded and cleaned\n",
    "- Chronological split (70/15/15)\n",
    "- Baseline model trained (TF-IDF + Logistic Regression)\n",
    "- Transformer fine-tuning available (DistilBERT)\n",
    "- Unsloth fine-tuning available (Llama-3.1-8B)\n",
    "- Results saved to `outputs/`\n",
    "\n",
    "**Note**: To run DistilBERT or Unsloth fine-tuning, uncomment the respective sections above.\n",
    "\n",
    "**Reproducibility**: All results use seed=319302\n",
    "\n",
    "**Data Leakage Prevention**: ✓ Chronological splits, transforms fit on train only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
